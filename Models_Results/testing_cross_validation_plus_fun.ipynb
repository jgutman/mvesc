{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "pathname = os.path.dirname(\"/home/jgutman/mvesc/Models_Results/\")\n",
    "full_pathname = os.path.abspath(pathname)\n",
    "split_pathname = full_pathname.split(sep=\"mvesc\")\n",
    "base_pathname = os.path.join(split_pathname[0], \"mvesc\")\n",
    "parentdir = os.path.join(base_pathname, \"ETL\")\n",
    "sys.path.insert(0,parentdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mvesc_utility_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all model import statements\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from sklearn.grid_search import ParameterGrid\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import *\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, confusion_matrix\n",
    "from sklearn.preprocessing import Imputer, StandardScaler, RobustScaler\n",
    "\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = \"\"\"select * from model.outcome\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11777\n"
     ]
    }
   ],
   "source": [
    "with postgres_pgconnection_generator() as connection:\n",
    "        with connection.cursor() as cursor:\n",
    "            cursor.execute(query)\n",
    "            results = cursor.fetchall()\n",
    "            print(len(results))\n",
    "        connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def df2num(rawdf):\n",
    "    \"\"\" Convert data frame with numeric variables and strings to numeric dataframe\n",
    "\n",
    "    :param pd.dataframe rawdf: raw data frame\n",
    "    :returns pd.dataframe df: a data frame with strings converted to dummies, other columns unchanged\n",
    "    :rtype: pd.dataframe\n",
    "    Rules:\n",
    "    - 1. numeric columns unchanged;\n",
    "    - 2. strings converted to dummeis;\n",
    "    - 3. the most frequent string is taken as reference\n",
    "    - 4. new column name is: \"ColumnName_Category\"\n",
    "    (e.g., column 'gender' with 80 'M' and 79 'F'; the dummy column left is 'gender_F')\n",
    "\n",
    "    \"\"\"\n",
    "    numeric_df = rawdf.select_dtypes(include=[np.number])\n",
    "    str_columns = [col for col in rawdf.columns if col not in numeric_df.columns]\n",
    "    dummy_col_df = pd.get_dummies(rawdf[str_columns], dummy_na=True)\n",
    "    numeric_df = numeric_df.join(dummy_col_df)\n",
    "    most_frequent_values = rawdf[str_columns].mode().loc[0].to_dict()\n",
    "    reference_cols = [\"{}_{}\".format(key, value) for key, value in most_frequent_values.items()]\n",
    "    numeric_df.drop(reference_cols, axis=1, inplace=True)\n",
    "    return numeric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def define_clfs_params():\n",
    "    # model_options[model_classes_selected] determines which of these models\n",
    "    # are actually run, all parameter options in grid run for each selected model\n",
    "\n",
    "    clfs = {\n",
    "        'logit': LogisticRegression(),\n",
    "        'LR_no_penalty': LogisticRegression(C=1e6),\n",
    "        'DT': DecisionTreeClassifier(),\n",
    "        'RF': RandomForestClassifier(n_estimators=50, n_jobs=-1),\n",
    "        'ET': ExtraTreesClassifier(n_estimators=10, n_jobs=-1, criterion='entropy'),\n",
    "        'AB': AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), algorithm=\"SAMME\", n_estimators=200),\n",
    "        'SVM': svm.SVC(kernel='linear', probability=False),\n",
    "        'GB': GradientBoostingClassifier(\n",
    "            learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=10),\n",
    "        'NB': GaussianNB(),\n",
    "        'SGD': SGDClassifier(loss=\"hinge\", penalty=\"l2\"),\n",
    "        'KNN': KNeighborsClassifier(n_neighbors=3)\n",
    "    }\n",
    "\n",
    "    grid = {\n",
    "        'logit': {'penalty': ['l1','l2'], 'C': [0.00001,0.0001,0.001,0.01,0.1,1.0,10.0]},\n",
    "        'LR_no_penalty': {},\n",
    "        'DT': {'criterion': ['gini', 'entropy'], 'max_depth': [1,5,10,20,50,100],\n",
    "            'max_features': ['sqrt','log2'],'min_samples_split': [2,5,10]},\n",
    "        'RF':{'n_estimators': [1,10,100,1000,10000], 'max_depth': [1,5,10,20,50,100],\n",
    "            'max_features': ['sqrt','log2'],'min_samples_split': [2,5,10]},\n",
    "        'SGD': {'loss': ['hinge','log','perceptron'], 'penalty': ['l2','l1','elasticnet']},\n",
    "        'ET': {'n_estimators': [1,10,100,1000,10000], 'criterion' : ['gini', 'entropy'] ,\n",
    "            'max_depth': [1,5,10,20,50,100], 'max_features': ['sqrt','log2'],'min_samples_split': [2,5,10]},\n",
    "        'AB': {'algorithm': ['SAMME', 'SAMME.R'], 'n_estimators': [1,10,100,1000,10000]},\n",
    "        'GB': {'n_estimators': [1,10,100,1000,10000], 'learning_rate' : [0.001,0.01,0.05,0.1,0.5],\n",
    "            'subsample' : [0.1,0.5,1.0], 'max_depth': [1,3,5,10,20,50,100]},\n",
    "        'NB' : {},\n",
    "        'DT': {'criterion': ['gini', 'entropy'], 'max_depth': [1,5,10,20,50,100],\n",
    "            'max_features': ['sqrt','log2'],'min_samples_split': [2,5,10]},\n",
    "        'SVM' :{'C' :[0.00001,0.0001,0.001,0.01,0.1,1,10],'kernel':['linear']},\n",
    "        'KNN' :{'n_neighbors': [1,5,10,25,50,100],'weights': ['uniform','distance'],\n",
    "            'algorithm': ['auto','ball_tree','kd_tree']}\n",
    "    }\n",
    "    return clfs, grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clf_loop(clfs, params, train_X, train_y,\n",
    "        criterion, models_to_run, cv_folds):\n",
    "    \"\"\"\n",
    "    Returns a dictionary where the keys are model nicknames (strings)\n",
    "    and the values are GridSearchCV objects containing attributes like\n",
    "    model.best_score_ and model.best_estimator_\n",
    "\n",
    "    :param dict(str:estimator) clfs: clfs as returned by define_clfs_params\n",
    "    :param dict(str:dict) params: grid of classifier hyperparameter options\n",
    "        to grid search over as returned by define_clfs_params\n",
    "    :param pandas.DataFrame train_X: index is student_lookup, columns are all\n",
    "        features to train over in the model\n",
    "    :param pandas.Series(int) train_y: index is student_lookup, value is 0 or 1\n",
    "        for outcome label\n",
    "    :param string criterion: evaluation criterion for model selection on the\n",
    "        validation set, to be read in from model_options (e.g. 'f1')\n",
    "    :param list[string] models_to_run: which models to actually run as read in\n",
    "        from model_options (e.g. ['logit', 'DT'])\n",
    "    :param sklearn.KFolds cv_folds: a KFolds generator object over the index\n",
    "        given in train_X and train_y (a list of lists of student_lookups)\n",
    "    :rtype dict(string: GridSearchCV)\n",
    "    \"\"\"\n",
    "    best_validated_models = dict()\n",
    "    for index,clf in enumerate([clfs[x] for x in models_to_run]):\n",
    "        model_name=models_to_run[index]\n",
    "        print(model_name)\n",
    "        parameter_values = params[model_name]\n",
    "        #param_grid = ParameterGrid(parameter_values)\n",
    "        best_validated_models[model_name] = GridSearchCV(clf, parameter_values, scoring=criterion, cv=cv_folds)\n",
    "        best_validated_models[model_name].fit(train_X, train_y)\n",
    "\n",
    "        model_cv_score = best_validated_models[model_name].best_score_\n",
    "        print(\"model: {model} cv_score: {score}\".format(\n",
    "            model=model_name, score=model_cv_score))\n",
    "    return best_validated_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def temporal_cohort_test_split(joint_df, cohort_grade_level_begin,\n",
    "    cohorts_held_out, cohorts_training):\n",
    "    \"\"\" Splits the given joint_df of features & outcomes and\n",
    "    returns a train/test dataset\n",
    "    :param pd.DataFrame joint_df:\n",
    "    :param list[int] cohorts_held_out:\n",
    "    \"\"\"\n",
    "    if (cohorts_training=='all'):\n",
    "        train = joint_df[~joint_df[cohort_grade_level_begin].isin(cohorts_held_out)]\n",
    "    else:\n",
    "        train = joint_df[joint_df[cohort_grade_level_begin].isin(cohorts_training)]\n",
    "    test = joint_df[joint_df[cohort_grade_level_begin].isin(cohorts_held_out)]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def measure_performance(outcomes, predictions):\n",
    "    \"\"\" Returns a dict of model performance objects\n",
    "    :param list[int] outcomes:\n",
    "    :param list[float] predictions:\n",
    "    \"\"\"\n",
    "    performance_objects = {}\n",
    "    performance_objects['pr_curve'] = precision_recall_curve(outcomes, predictions)\n",
    "    performance_objects['roc_curve'] = roc_curve(outcomes, predictions)\n",
    "    #performance_objects['confusion_matrix'] = confusion_matrix(outcomes,predictions)\n",
    "    return performance_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_outcomes_plus_features(model_options):\n",
    "    with postgres_pgconnection_generator() as connection:\n",
    "        # get labeled outcomes\n",
    "        # Assumes:\n",
    "        # model.outcome table contains a column (name given in cohort_grade_level_begin) for each cohort base year we choose\n",
    "        # e.g. 'cohort_9th' contains the year each student is seen in 9th grade\n",
    "        # and contains an outcome column (name given in outcome_name)\n",
    "        # and 'student_lookup' columns\n",
    "        # Usage:\n",
    "        # select train, validation, and test based on values in column\n",
    "        # 'cohort_grade_level_begin' according to value in 'cohorts_held_out'\n",
    "        outcomes_with_student_lookup = read_table_to_df(connection,\n",
    "            table_name = 'outcome', schema = 'model', nrows = -1,\n",
    "            columns = ['student_lookup', model_options['outcome_name'], model_options['cohort_grade_level_begin']])\n",
    "        # drop students without student_lookup, outcome, or cohort identifier\n",
    "        # can use subset = [colnames] to drop based on NAs in certain columns only\n",
    "        outcomes_with_student_lookup.dropna(inplace=True)\n",
    "        joint_label_features = outcomes_with_student_lookup.copy()\n",
    "\n",
    "        # get all requested input features\n",
    "        # Assumes:\n",
    "        # every features table contains 'student_lookup'\n",
    "        # plus a column for the requested possible features\n",
    "\n",
    "        for table, column_names in model_options['features_included'].items():\n",
    "            features = read_table_to_df(connection, table_name = table,\n",
    "                schema = 'model', nrows = -1,\n",
    "                columns=(['student_lookup'] + column_names))\n",
    "        # join to only keep features that have labeled outcomes\n",
    "            joint_label_features = pd.merge(joint_label_features, features,\n",
    "                how = 'left', on = 'student_lookup')\n",
    "\n",
    "    # build dataframe containing student_lookup, outcome, cohort,\n",
    "    # and all features as numeric non-categorical values\n",
    "    joint_label_features.set_index('student_lookup', inplace=True)\n",
    "    joint_label_features = df2num(joint_label_features)\n",
    "    return joint_label_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_in_yaml(filename=os.path.join(base_pathname,\n",
    "        'Models_Results', 'model_options.yaml')):\n",
    "    with open(filename, 'r') as f:\n",
    "        model_options = yaml.load(f)\n",
    "\n",
    "    # Maybe we want to have default values for these options and replace\n",
    "    # from a new yaml file as necessary\n",
    "    assert(type(model_options) == dict), \"bad formatting in yaml file\"\n",
    "    required_keys = set(('validation_criterion', 'features_included', 'cohorts_training',\n",
    "        'cohorts_held_out', 'file_save_name', 'model_classes_selected', 'outcome_name',\n",
    "        'cohort_grade_level_begin', 'model_test_holdout', 'random_seed'))\n",
    "    assert(all([key in model_options.keys() for key in required_keys])), \\\n",
    "        \"missing model specifications in yaml file\"\n",
    "\n",
    "    assert(type(model_options['features_included']) == dict), \"bad formatting in yaml file\"\n",
    "    assert(type(model_options['model_classes_selected']) == list), \"bad formatting in yaml file\"\n",
    "    assert(type(model_options['cohorts_held_out']) == list), \"bad formatting in yaml file\"\n",
    "    assert(type(model_options['cohorts_training']) == list or\n",
    "        model_options['cohorts_training'] == 'all'), \"bad formatting in yaml file\"\n",
    "    return model_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'missing_impute_strategy': 'mean_plus_dummies', 'cohorts_training': 'all', 'cohorts_held_out': [2012], 'user_description': 'initial_skeleton_pipeline_test', 'parameter_cross_validation_scheme': 'leave_cohort_out', 'feature_scaling': 'none', 'file_save_name': 'gender_ethnicity', 'features_included': {'demographics': ['ethnicity', 'gender']}, 'cohort_grade_level_begin': 'cohort_9th', 'random_seed': 2187, 'model_test_holdout': 'temporal_cohort', 'n_folds': 10, 'model_classes_selected': ['logit', 'DT'], 'validation_criterion': 'accuracy', 'outcome_name': 'not_on_time'}\n"
     ]
    }
   ],
   "source": [
    "model_options = read_in_yaml()\n",
    "print(model_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set seed for this program from model_options\n",
    "np.random.seed(model_options['random_seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_options['features_included'] = {'demographics': ['ethnicity', 'gender'], \n",
    "                                      'grades': ['gpa_gr_7', 'gpa_gr_8', 'gpa_gr_9']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Based on options, draw in data and select the appropriate\n",
    "# labeled outcome column (outcome_name)\n",
    "# cohort identification column (cohort_grade_level_begin)\n",
    "# subset of various feature columns from various tables (features_included)\n",
    "\n",
    "outcome_plus_features = build_outcomes_plus_features(model_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>not_on_time</th>\n",
       "      <th>cohort_9th</th>\n",
       "      <th>gpa_gr_7</th>\n",
       "      <th>gpa_gr_8</th>\n",
       "      <th>gpa_gr_9</th>\n",
       "      <th>ethnicity_A</th>\n",
       "      <th>ethnicity_B</th>\n",
       "      <th>ethnicity_H</th>\n",
       "      <th>ethnicity_I</th>\n",
       "      <th>ethnicity_M</th>\n",
       "      <th>ethnicity_nan</th>\n",
       "      <th>gender_F</th>\n",
       "      <th>gender_nan</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>student_lookup</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57296.0</th>\n",
       "      <td>0</td>\n",
       "      <td>2006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.266667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58652.0</th>\n",
       "      <td>0</td>\n",
       "      <td>2006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.529032</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57294.0</th>\n",
       "      <td>0</td>\n",
       "      <td>2006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.205000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69065.0</th>\n",
       "      <td>1</td>\n",
       "      <td>2006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63909.0</th>\n",
       "      <td>1</td>\n",
       "      <td>2006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                not_on_time  cohort_9th  gpa_gr_7  gpa_gr_8  gpa_gr_9  \\\n",
       "student_lookup                                                          \n",
       "57296.0                   0        2006       NaN       NaN  1.266667   \n",
       "58652.0                   0        2006       NaN       NaN  3.529032   \n",
       "57294.0                   0        2006       NaN       NaN  1.205000   \n",
       "69065.0                   1        2006       NaN       NaN       NaN   \n",
       "63909.0                   1        2006       NaN       NaN       NaN   \n",
       "\n",
       "                ethnicity_A  ethnicity_B  ethnicity_H  ethnicity_I  \\\n",
       "student_lookup                                                       \n",
       "57296.0                 0.0          0.0          0.0          0.0   \n",
       "58652.0                 0.0          0.0          0.0          0.0   \n",
       "57294.0                 0.0          0.0          0.0          0.0   \n",
       "69065.0                 0.0          0.0          0.0          0.0   \n",
       "63909.0                 0.0          1.0          0.0          0.0   \n",
       "\n",
       "                ethnicity_M  ethnicity_nan  gender_F  gender_nan  \n",
       "student_lookup                                                    \n",
       "57296.0                 0.0            0.0       0.0         0.0  \n",
       "58652.0                 0.0            0.0       0.0         0.0  \n",
       "57294.0                 0.0            0.0       0.0         0.0  \n",
       "69065.0                 0.0            0.0       0.0         0.0  \n",
       "63909.0                 0.0            0.0       0.0         0.0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outcome_plus_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if model_options['model_test_holdout'] == 'temporal_cohort':\n",
    "    # if using temporal cohort model performance validation,\n",
    "    # we choose the cohorts in cohorts_held_out for the test set\n",
    "    train, test = temporal_cohort_test_split(outcome_plus_features,\n",
    "        model_options['cohort_grade_level_begin'],\n",
    "        model_options['cohorts_held_out'],\n",
    "        model_options['cohorts_training'])\n",
    "\n",
    "else:\n",
    "    # if not using temporal test set, split randomly\n",
    "    train, test = train_test_split(outcome_plus_features, test_size=0.20,\n",
    "        random_state=model_options['random_seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2006 2007 2008 2009 2010 2011]\n",
      "[2012]\n"
     ]
    }
   ],
   "source": [
    "print(pd.unique(train.cohort_9th))\n",
    "print(pd.unique(test.cohort_9th))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get subtables for each for easy reference\n",
    "train_X = train.drop([model_options['outcome_name'],\n",
    "    model_options['cohort_grade_level_begin']],axis=1)\n",
    "test_X = test.drop([model_options['outcome_name'],\n",
    "    model_options['cohort_grade_level_begin']],axis=1)\n",
    "train_y = train[model_options['outcome_name']]\n",
    "test_y = test[model_options['outcome_name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clfs, params = define_clfs_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leave_cohort_out\n"
     ]
    }
   ],
   "source": [
    "if model_options['parameter_cross_validation_scheme'] == 'none':\n",
    "    # no need to further manipulate train dataset\n",
    "    cohort_kfolds = 2 # hacky way to have GridSearchCV fit to 2 k-folds\n",
    "elif model_options['parameter_cross_validation_scheme'] == 'leave_cohort_out':\n",
    "    # choose another validation set amongst the training set to\n",
    "    # estimate parameters and model selection across cohort folds\n",
    "    print('leave_cohort_out')\n",
    "    cohort_kfolds = LeaveOneLabelOut(train[model_options['cohort_grade_level_begin']])\n",
    "elif model_options['parameter_cross_validation_scheme'] == 'k_fold':\n",
    "    # ignore cohorts and use random folds to estimate parameter\n",
    "    print('k_fold_parameter_estimation')\n",
    "    cohort_kfolds = LabelKFold(train.index, n_folds=model_options['n_folds'])\n",
    "else:\n",
    "    print('unknown cross-validation strategy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2006 2006 2006 ..., 2011 2011 2011]\n",
      "train:  [2007 2008 2009 2010 2011]\n",
      "validation:  [2006]\n",
      "train:  [2006 2008 2009 2010 2011]\n",
      "validation:  [2007]\n",
      "train:  [2006 2007 2009 2010 2011]\n",
      "validation:  [2008]\n",
      "train:  [2006 2007 2008 2010 2011]\n",
      "validation:  [2009]\n",
      "train:  [2006 2007 2008 2009 2011]\n",
      "validation:  [2010]\n",
      "train:  [2006 2007 2008 2009 2010]\n",
      "validation:  [2011]\n"
     ]
    }
   ],
   "source": [
    "print(cohort_kfolds.labels)\n",
    "for train_fold, val_fold in cohort_kfolds:\n",
    "    print('train: ', np.unique(train.iloc[train_fold].cohort_9th))\n",
    "    print('validation: ', np.unique(train.iloc[val_fold].cohort_9th))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_options['model_classes_selected'] = ['LR_no_penalty'] #'log_loss' #'accuracy' #  # 'f1'\n",
    "model_options['validation_criterion'] = 'average_precision' #'log_loss' #'accuracy' #  # 'f1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AB': AdaBoostClassifier(algorithm='SAMME',\n",
       "           base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
       "             max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=None, splitter='best'),\n",
       "           learning_rate=1.0, n_estimators=200, random_state=None),\n",
       " 'DT': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "             max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=None, splitter='best'),\n",
       " 'ET': ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       " 'GB': GradientBoostingClassifier(init=None, learning_rate=0.05, loss='deviance',\n",
       "               max_depth=6, max_features=None, max_leaf_nodes=None,\n",
       "               min_samples_leaf=1, min_samples_split=2,\n",
       "               min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "               presort='auto', random_state=None, subsample=0.5, verbose=0,\n",
       "               warm_start=False),\n",
       " 'KNN': KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "            metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
       "            weights='uniform'),\n",
       " 'LR_no_penalty': LogisticRegression(C=1000000.0, class_weight=None, dual=False,\n",
       "           fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "           multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "           solver='liblinear', tol=0.0001, verbose=0, warm_start=False),\n",
       " 'NB': GaussianNB(),\n",
       " 'RF': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "             max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=-1,\n",
       "             oob_score=False, random_state=None, verbose=0,\n",
       "             warm_start=False),\n",
       " 'SGD': SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "        eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "        learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=1,\n",
       "        penalty='l2', power_t=0.5, random_state=None, shuffle=True,\n",
       "        verbose=0, warm_start=False),\n",
       " 'SVM': SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "   decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
       "   max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "   tol=0.001, verbose=False),\n",
       " 'logit': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False)}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR_no_penalty\n",
      "model: LR_no_penalty cv_score: 0.34711971229428645\n"
     ]
    }
   ],
   "source": [
    "best_validated_models = clf_loop(clfs, params, train_X, train_y,\n",
    "    criterion=model_options['validation_criterion'],\n",
    "    models_to_run=model_options['model_classes_selected'],\n",
    "    cv_folds=cohort_kfolds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for model_name, model in best_validated_models.items():\n",
    "    clf = model.best_estimator_\n",
    "    if hasattr(clf, \"decision_function\"):\n",
    "        test_set_scores = clf.decision_function(test_X)\n",
    "    else:\n",
    "        test_set_scores = clf.predict_proba(test_X)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mean: 0.34712, std: 0.02759, params: {}]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_validated_models['LR_no_penalty'].grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.69831928,  1.14873057,  1.11487351,  1.52102882,  0.75204177,\n",
       "         2.6156949 , -0.14418855,  0.        ]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_validated_models['LR_no_penalty'].best_estimator_.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['not_on_time', 'cohort_9th', 'ethnicity_A', 'ethnicity_B',\n",
       "       'ethnicity_H', 'ethnicity_I', 'ethnicity_M', 'ethnicity_nan',\n",
       "       'gender_F', 'gender_nan'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outcome_plus_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_train = best_validated_models['DT'].predict_proba(train_X)\n",
    "predicted_test = best_validated_models['DT'].predict_proba(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([probs == [0.5, 0.5] for probs in predicted_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([probs == [0.5, 0.5] for probs in predicted_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = temporal_cohort_test_split(outcome_plus_features,\n",
    "    model_options['cohort_grade_level_begin'],\n",
    "    model_options['cohorts_held_out'],\n",
    "    model_options['cohorts_training'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['validation_criterion', 'n_folds', 'features_included', 'user_description', 'cohorts_training', 'cohort_grade_level_begin', 'random_seed', 'missing_impute_strategy', 'feature_scaling', 'model_test_holdout', 'file_save_name', 'model_classes_selected', 'cohorts_held_out', 'parameter_cross_validation_scheme', 'outcome_name'])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_options.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_null_dummies(data):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    data_null_columns = data[data.columns[data.isnull().sum() > 0]]\n",
    "    data_null_dummies = data_null_columns.isnull()*1.0\n",
    "    data_null_dummies.rename(columns=lambda x: x + '_isnull', inplace=True)\n",
    "    data_plus_dummies = data.merge(data_null_dummies, left_index=True, right_index=True)\n",
    "    return data_plus_dummies\n",
    "\n",
    "def impute_missing_values(train, test, strategy):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if (strategy=='none'):\n",
    "        return train, test\n",
    "        \n",
    "    elif(strategy == 'mean_plus_dummies' or strategy == 'median_plus_dummies'):\n",
    "        train = add_null_dummies(train) # add feature_isnull columns 0 or 1\n",
    "        test = add_null_dummies(test)\n",
    "\n",
    "        imputer = Imputer(strategy=strategy.split(\"_\")[0])\n",
    "        imputer.fit(train)\n",
    "        train = pd.DataFrame(imputer.transform(train), columns = train.columns, index = train.index)\n",
    "        test = pd.DataFrame(imputer.transform(test), columns = test.columns, index = test.index)\n",
    "        return train, test\n",
    "\n",
    "    else:\n",
    "        print('unknown imputation strategy. try \"{}\", \"{}\", or \"{}\"'.format(\n",
    "            'mean_plus_dummies', 'median_plus_dummies', 'none'))\n",
    "        return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scale_features(train, test, strategy):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    num_values_by_column = {x: len(train[x].unique()) for x in train.columns}\n",
    "    zero_variance_columns = [k for k,v in num_values_by_column.items() if v == 1]\n",
    "    train.drop(zero_variance_columns, axis=1, inplace=True)\n",
    "    test.drop(zero_variance_columns, axis=1, inplace=True)\n",
    "\n",
    "    if (strategy == 'none'):\n",
    "        return train, test\n",
    "        \n",
    "    elif(strategy == 'standard' or strategy == 'robust'):\n",
    "        non_binary_columns = [k for k, v in num_values_by_column.items() if v > 2]\n",
    "        scaler = StandardScaler() if strategy == 'standard' else RobustScaler()\n",
    "        train_non_binary = train[non_binary_columns]\n",
    "        test_non_binary = test[non_binary_columns]\n",
    "        scaler.fit(train_non_binary)\n",
    "        train_non_binary = pd.DataFrame(scaler.transform(train_non_binary),\n",
    "            columns = non_binary_columns, index = train.index)\n",
    "        test_non_binary = pd.DataFrame(scaler.transform(test_non_binary),\n",
    "            columns = non_binary_columns, index = test.index)\n",
    "\n",
    "        train_scaled = train.drop(non_binary_columns, axis=1)\n",
    "        test_scaled = test.drop(non_binary_columns, axis=1)\n",
    "        train_scaled = train_scaled.merge(train_non_binary,\n",
    "            left_index=True, right_index=True)\n",
    "        test_scaled = test_scaled.merge(test_non_binary,\n",
    "            left_index=True, right_index=True)\n",
    "        return train_scaled, test_scaled\n",
    "\n",
    "    else:\n",
    "        print('unknown feature scaling strategy. try \"{}\", \"{}\", or \"{}\"'.format(\n",
    "            'standard', 'robust', 'none'))\n",
    "        return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_imputed, test_imputed = impute_missing_values(train_X, test_X, 'median_plus_dummies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get subtables for each for easy reference\n",
    "train_X = train.drop([model_options['outcome_name'],\n",
    "    model_options['cohort_grade_level_begin']],axis=1)\n",
    "test_X = test.drop([model_options['outcome_name'],\n",
    "    model_options['cohort_grade_level_begin']],axis=1)\n",
    "train_y = train[model_options['outcome_name']]\n",
    "test_y = test[model_options['outcome_name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_values_by_column = {x: len(train_X[x].unique()) for x in train_X.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_scaled, test_scaled = scale_features(train_imputed, test_imputed, 'robust')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# do missing value feature imputation here\n",
    "train_X, test_X = impute_missing_values(train_X, test_X, model_options['missing_impute_strategy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert(all(train_X.columns == test_X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'none'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_options['feature_scaling']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ethnicity_A</th>\n",
       "      <th>ethnicity_B</th>\n",
       "      <th>ethnicity_H</th>\n",
       "      <th>ethnicity_I</th>\n",
       "      <th>ethnicity_M</th>\n",
       "      <th>ethnicity_nan</th>\n",
       "      <th>gender_F</th>\n",
       "      <th>gpa_gr_7_isnull</th>\n",
       "      <th>gpa_gr_8_isnull</th>\n",
       "      <th>gpa_gr_9_isnull</th>\n",
       "      <th>gpa_gr_7</th>\n",
       "      <th>gpa_gr_8</th>\n",
       "      <th>gpa_gr_9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>student_lookup</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57296.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.954470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58652.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.012567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57294.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.035344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69065.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63909.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57292.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57290.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57288.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57285.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57284.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.340262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57282.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41726.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.030608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57279.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.351541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57278.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57277.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.198536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57276.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57275.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.387607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57274.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57273.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57271.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57270.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.433508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57268.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.285967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57266.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57265.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.663017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57264.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.366492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57259.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.536553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58523.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.697639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36739.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.650074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57255.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57254.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.197443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701016.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701036.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701037.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701041.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701046.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701056.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701058.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701067.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701084.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701090.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701091.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701093.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701097.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701098.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701099.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701100.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701101.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701105.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701108.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701110.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701115.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701121.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701130.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701139.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701145.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701148.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701157.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701163.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701165.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701168.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9077 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                ethnicity_A  ethnicity_B  ethnicity_H  ethnicity_I  \\\n",
       "student_lookup                                                       \n",
       "57296.0                 0.0          0.0          0.0          0.0   \n",
       "58652.0                 0.0          0.0          0.0          0.0   \n",
       "57294.0                 0.0          0.0          0.0          0.0   \n",
       "69065.0                 0.0          0.0          0.0          0.0   \n",
       "63909.0                 0.0          1.0          0.0          0.0   \n",
       "57292.0                 0.0          0.0          0.0          0.0   \n",
       "57290.0                 0.0          0.0          0.0          0.0   \n",
       "57288.0                 0.0          0.0          0.0          0.0   \n",
       "57285.0                 0.0          0.0          0.0          0.0   \n",
       "57284.0                 0.0          0.0          0.0          0.0   \n",
       "57282.0                 0.0          0.0          0.0          0.0   \n",
       "41726.0                 0.0          0.0          0.0          0.0   \n",
       "57279.0                 0.0          0.0          0.0          0.0   \n",
       "57278.0                 0.0          0.0          0.0          0.0   \n",
       "57277.0                 0.0          0.0          0.0          0.0   \n",
       "57276.0                 0.0          0.0          0.0          0.0   \n",
       "57275.0                 0.0          0.0          0.0          0.0   \n",
       "57274.0                 0.0          0.0          0.0          0.0   \n",
       "57273.0                 0.0          0.0          0.0          0.0   \n",
       "57271.0                 0.0          0.0          0.0          0.0   \n",
       "57270.0                 0.0          0.0          0.0          0.0   \n",
       "57268.0                 0.0          0.0          0.0          0.0   \n",
       "57266.0                 0.0          0.0          0.0          0.0   \n",
       "57265.0                 0.0          0.0          0.0          0.0   \n",
       "57264.0                 0.0          0.0          0.0          0.0   \n",
       "57259.0                 0.0          0.0          0.0          0.0   \n",
       "58523.0                 0.0          0.0          0.0          0.0   \n",
       "36739.0                 0.0          0.0          0.0          0.0   \n",
       "57255.0                 0.0          0.0          0.0          0.0   \n",
       "57254.0                 0.0          0.0          0.0          0.0   \n",
       "...                     ...          ...          ...          ...   \n",
       "701016.0                0.0          0.0          0.0          0.0   \n",
       "701036.0                0.0          0.0          0.0          0.0   \n",
       "701037.0                0.0          0.0          0.0          0.0   \n",
       "701041.0                0.0          0.0          0.0          0.0   \n",
       "701046.0                0.0          0.0          0.0          0.0   \n",
       "701056.0                0.0          0.0          0.0          0.0   \n",
       "701058.0                0.0          0.0          0.0          0.0   \n",
       "701067.0                0.0          0.0          0.0          0.0   \n",
       "701084.0                0.0          0.0          0.0          0.0   \n",
       "701090.0                0.0          0.0          0.0          0.0   \n",
       "701091.0                0.0          0.0          0.0          0.0   \n",
       "701093.0                0.0          0.0          0.0          0.0   \n",
       "701097.0                0.0          0.0          0.0          0.0   \n",
       "701098.0                0.0          0.0          0.0          0.0   \n",
       "701099.0                0.0          0.0          0.0          0.0   \n",
       "701100.0                0.0          0.0          0.0          0.0   \n",
       "701101.0                0.0          0.0          0.0          0.0   \n",
       "701105.0                0.0          0.0          0.0          0.0   \n",
       "701108.0                0.0          0.0          0.0          0.0   \n",
       "701110.0                0.0          0.0          0.0          0.0   \n",
       "701115.0                0.0          0.0          0.0          0.0   \n",
       "701121.0                0.0          0.0          0.0          0.0   \n",
       "701130.0                0.0          0.0          0.0          0.0   \n",
       "701139.0                0.0          1.0          0.0          0.0   \n",
       "701145.0                0.0          0.0          0.0          0.0   \n",
       "701148.0                0.0          0.0          0.0          0.0   \n",
       "701157.0                0.0          1.0          0.0          0.0   \n",
       "701163.0                0.0          0.0          0.0          0.0   \n",
       "701165.0                0.0          0.0          0.0          0.0   \n",
       "701168.0                0.0          0.0          0.0          0.0   \n",
       "\n",
       "                ethnicity_M  ethnicity_nan  gender_F  gpa_gr_7_isnull  \\\n",
       "student_lookup                                                          \n",
       "57296.0                 0.0            0.0       0.0              1.0   \n",
       "58652.0                 0.0            0.0       0.0              1.0   \n",
       "57294.0                 0.0            0.0       0.0              1.0   \n",
       "69065.0                 0.0            0.0       0.0              1.0   \n",
       "63909.0                 0.0            0.0       0.0              1.0   \n",
       "57292.0                 0.0            0.0       1.0              1.0   \n",
       "57290.0                 0.0            0.0       1.0              1.0   \n",
       "57288.0                 0.0            0.0       0.0              1.0   \n",
       "57285.0                 0.0            0.0       0.0              1.0   \n",
       "57284.0                 0.0            0.0       1.0              1.0   \n",
       "57282.0                 0.0            0.0       1.0              1.0   \n",
       "41726.0                 0.0            0.0       1.0              1.0   \n",
       "57279.0                 0.0            0.0       0.0              1.0   \n",
       "57278.0                 0.0            0.0       1.0              1.0   \n",
       "57277.0                 0.0            0.0       1.0              1.0   \n",
       "57276.0                 0.0            0.0       0.0              1.0   \n",
       "57275.0                 0.0            0.0       0.0              1.0   \n",
       "57274.0                 0.0            0.0       1.0              1.0   \n",
       "57273.0                 0.0            0.0       1.0              1.0   \n",
       "57271.0                 0.0            0.0       0.0              1.0   \n",
       "57270.0                 0.0            0.0       1.0              1.0   \n",
       "57268.0                 0.0            0.0       1.0              1.0   \n",
       "57266.0                 0.0            0.0       1.0              1.0   \n",
       "57265.0                 0.0            0.0       1.0              1.0   \n",
       "57264.0                 0.0            0.0       0.0              1.0   \n",
       "57259.0                 0.0            0.0       1.0              1.0   \n",
       "58523.0                 0.0            0.0       1.0              1.0   \n",
       "36739.0                 0.0            0.0       0.0              1.0   \n",
       "57255.0                 1.0            0.0       0.0              1.0   \n",
       "57254.0                 0.0            0.0       0.0              1.0   \n",
       "...                     ...            ...       ...              ...   \n",
       "701016.0                0.0            0.0       0.0              1.0   \n",
       "701036.0                0.0            0.0       0.0              1.0   \n",
       "701037.0                0.0            0.0       0.0              1.0   \n",
       "701041.0                0.0            0.0       1.0              1.0   \n",
       "701046.0                0.0            0.0       1.0              1.0   \n",
       "701056.0                0.0            0.0       0.0              1.0   \n",
       "701058.0                0.0            0.0       1.0              1.0   \n",
       "701067.0                0.0            0.0       0.0              1.0   \n",
       "701084.0                0.0            0.0       0.0              1.0   \n",
       "701090.0                0.0            0.0       0.0              1.0   \n",
       "701091.0                1.0            0.0       0.0              1.0   \n",
       "701093.0                0.0            0.0       0.0              1.0   \n",
       "701097.0                0.0            0.0       0.0              1.0   \n",
       "701098.0                0.0            0.0       0.0              1.0   \n",
       "701099.0                0.0            0.0       1.0              1.0   \n",
       "701100.0                0.0            0.0       1.0              1.0   \n",
       "701101.0                0.0            0.0       1.0              1.0   \n",
       "701105.0                0.0            0.0       1.0              1.0   \n",
       "701108.0                0.0            0.0       0.0              1.0   \n",
       "701110.0                0.0            0.0       0.0              1.0   \n",
       "701115.0                0.0            0.0       0.0              1.0   \n",
       "701121.0                0.0            0.0       1.0              1.0   \n",
       "701130.0                0.0            0.0       0.0              1.0   \n",
       "701139.0                0.0            0.0       0.0              1.0   \n",
       "701145.0                0.0            0.0       0.0              1.0   \n",
       "701148.0                0.0            0.0       0.0              1.0   \n",
       "701157.0                0.0            0.0       0.0              1.0   \n",
       "701163.0                0.0            0.0       1.0              1.0   \n",
       "701165.0                0.0            0.0       0.0              1.0   \n",
       "701168.0                0.0            0.0       1.0              1.0   \n",
       "\n",
       "                gpa_gr_8_isnull  gpa_gr_9_isnull  gpa_gr_7  gpa_gr_8  gpa_gr_9  \n",
       "student_lookup                                                                  \n",
       "57296.0                     1.0              0.0       0.0       0.0 -1.954470  \n",
       "58652.0                     1.0              0.0       0.0       0.0  1.012567  \n",
       "57294.0                     1.0              0.0       0.0       0.0 -2.035344  \n",
       "69065.0                     1.0              1.0       0.0       0.0  0.000000  \n",
       "63909.0                     1.0              1.0       0.0       0.0  0.000000  \n",
       "57292.0                     1.0              1.0       0.0       0.0  0.000000  \n",
       "57290.0                     1.0              1.0       0.0       0.0  0.000000  \n",
       "57288.0                     1.0              1.0       0.0       0.0  0.000000  \n",
       "57285.0                     1.0              1.0       0.0       0.0  0.000000  \n",
       "57284.0                     1.0              0.0       0.0       0.0 -1.340262  \n",
       "57282.0                     1.0              1.0       0.0       0.0  0.000000  \n",
       "41726.0                     1.0              0.0       0.0       0.0 -1.030608  \n",
       "57279.0                     1.0              0.0       0.0       0.0  0.351541  \n",
       "57278.0                     1.0              1.0       0.0       0.0  0.000000  \n",
       "57277.0                     1.0              0.0       0.0       0.0  1.198536  \n",
       "57276.0                     1.0              1.0       0.0       0.0  0.000000  \n",
       "57275.0                     1.0              0.0       0.0       0.0  1.387607  \n",
       "57274.0                     1.0              1.0       0.0       0.0  0.000000  \n",
       "57273.0                     1.0              1.0       0.0       0.0  0.000000  \n",
       "57271.0                     1.0              1.0       0.0       0.0  0.000000  \n",
       "57270.0                     1.0              0.0       0.0       0.0  1.433508  \n",
       "57268.0                     1.0              0.0       0.0       0.0  0.285967  \n",
       "57266.0                     1.0              1.0       0.0       0.0  0.000000  \n",
       "57265.0                     1.0              0.0       0.0       0.0  0.663017  \n",
       "57264.0                     1.0              0.0       0.0       0.0 -1.366492  \n",
       "57259.0                     1.0              0.0       0.0       0.0  1.536553  \n",
       "58523.0                     1.0              0.0       0.0       0.0 -0.697639  \n",
       "36739.0                     1.0              0.0       0.0       0.0  0.650074  \n",
       "57255.0                     1.0              1.0       0.0       0.0  0.000000  \n",
       "57254.0                     1.0              0.0       0.0       0.0  1.197443  \n",
       "...                         ...              ...       ...       ...       ...  \n",
       "701016.0                    1.0              1.0       0.0       0.0  0.000000  \n",
       "701036.0                    1.0              1.0       0.0       0.0  0.000000  \n",
       "701037.0                    1.0              1.0       0.0       0.0  0.000000  \n",
       "701041.0                    1.0              1.0       0.0       0.0  0.000000  \n",
       "701046.0                    1.0              1.0       0.0       0.0  0.000000  \n",
       "701056.0                    1.0              1.0       0.0       0.0  0.000000  \n",
       "701058.0                    1.0              1.0       0.0       0.0  0.000000  \n",
       "701067.0                    1.0              1.0       0.0       0.0  0.000000  \n",
       "701084.0                    1.0              1.0       0.0       0.0  0.000000  \n",
       "701090.0                    1.0              1.0       0.0       0.0  0.000000  \n",
       "701091.0                    1.0              1.0       0.0       0.0  0.000000  \n",
       "701093.0                    1.0              1.0       0.0       0.0  0.000000  \n",
       "701097.0                    1.0              1.0       0.0       0.0  0.000000  \n",
       "701098.0                    1.0              1.0       0.0       0.0  0.000000  \n",
       "701099.0                    1.0              1.0       0.0       0.0  0.000000  \n",
       "701100.0                    1.0              1.0       0.0       0.0  0.000000  \n",
       "701101.0                    1.0              1.0       0.0       0.0  0.000000  \n",
       "701105.0                    1.0              1.0       0.0       0.0  0.000000  \n",
       "701108.0                    1.0              1.0       0.0       0.0  0.000000  \n",
       "701110.0                    1.0              1.0       0.0       0.0  0.000000  \n",
       "701115.0                    1.0              1.0       0.0       0.0  0.000000  \n",
       "701121.0                    1.0              1.0       0.0       0.0  0.000000  \n",
       "701130.0                    1.0              1.0       0.0       0.0  0.000000  \n",
       "701139.0                    1.0              1.0       0.0       0.0  0.000000  \n",
       "701145.0                    1.0              1.0       0.0       0.0  0.000000  \n",
       "701148.0                    1.0              1.0       0.0       0.0  0.000000  \n",
       "701157.0                    1.0              1.0       0.0       0.0  0.000000  \n",
       "701163.0                    1.0              1.0       0.0       0.0  0.000000  \n",
       "701165.0                    1.0              1.0       0.0       0.0  0.000000  \n",
       "701168.0                    1.0              1.0       0.0       0.0  0.000000  \n",
       "\n",
       "[9077 rows x 13 columns]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
