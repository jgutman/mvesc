{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "parentdir = os.path.abspath('/home/xcheng/mvesc/ETL')\n",
    "sys.path.insert(0,parentdir)\n",
    "from mvesc_utility_functions import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "from numpy import random\n",
    "import pickle\n",
    "from estimate_prediction_model import *\n",
    "from write_to_database import write_scores_to_db\n",
    "from optparse import OptionParser\n",
    "import re\n",
    "%load_ext autotime\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 49.9 ms\n"
     ]
    }
   ],
   "source": [
    "tab_reports = 'model.reports' # it has batch_name, precision, recall\n",
    "tab_feature_scores = 'model.feature_scores'\n",
    "tab_prediction = 'model.predictions'\n",
    "dir_pkls = '/mnt/data/mvesc/Models_Results/pkls'\n",
    "pkls_logit = ['08_12_2016_grade_7_param_set_17_logit_ht_19082']\n",
    "pkls_logit = os.path.join(dir_pkls, pkls_logit[0])+'_logit.pkl'\n",
    "with open(pkls_logit, 'rb') as handle:\n",
    "    pkl = pickle.load(handle)\n",
    "    \n",
    "with open(dir_pkls+'/08_12_2016_grade_7_param_set_17_logit_ht_19078_logit.pkl', 'rb') as handle:\n",
    "    pkl2 = pickle.load(handle)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cross_validation_scores': array([ 0.24293077,  0.2596474 ]),\n",
       " 'estimator': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False),\n",
       " 'features': Index(['num_stem_classes_gr_5', 'num_future_prep_classes_gr_6',\n",
       "        'num_health_classes_gr_5', 'percent_passed_pf_classes_gr_5', 'iss_gr_5',\n",
       "        'avg_address_change_to_gr_5', 'n_districts_to_gr_5',\n",
       "        'n_records_to_gr_5', 'avg_district_change_to_gr_5',\n",
       "        'n_addresses_to_gr_5',\n",
       "        ...\n",
       "        'n_records_to_gr_6', 'num_humanities_classes_gr_6',\n",
       "        'avg_city_change_to_gr_6', 'oss_gr_5', 'math_normalized_gr_5',\n",
       "        'num_pf_classes_gr_5', 'num_art_classes_gr_5', 'gpa_gr_6',\n",
       "        'stem_gpa_gr_5', 'num_health_classes_gr_6'],\n",
       "       dtype='object', length=188),\n",
       " 'file_name': '08_12_2016_grade_7_param_set_17_logit_ht_19082',\n",
       " 'model_name': 'logit',\n",
       " 'model_options': {'batch_name': '08_12_2016_grade_7',\n",
       "  'cohort_grade_level_begin': 'cohort_7th',\n",
       "  'cohorts_test': [2010],\n",
       "  'cohorts_training': [2008],\n",
       "  'cohorts_val': [2009],\n",
       "  'debug': False,\n",
       "  'downsample_param': 0.8,\n",
       "  'feature_grade_range': [5, 6],\n",
       "  'feature_scaling': 'robust',\n",
       "  'features_included': {'absence': ['tardy_gr_6',\n",
       "    'tardy_wkd_3_gr_6',\n",
       "    'medical_gr_5',\n",
       "    'absence_wkd_1_gr_6',\n",
       "    'tardy_gr_5',\n",
       "    'tardy_consec_gr_5',\n",
       "    'absence_unexcused_gr_6',\n",
       "    'medical_gr_6',\n",
       "    'absence_wkd_5_gr_6',\n",
       "    'tardy_wkd_1_gr_5',\n",
       "    'absence_wkd_2_gr_6',\n",
       "    'tardy_consec_gr_6',\n",
       "    'tardy_unexcused_gr_6',\n",
       "    'tardy_wkd_4_gr_6',\n",
       "    'tardy_wkd_2_gr_6',\n",
       "    'absence_wkd_3_gr_6',\n",
       "    'absence_consec_gr_6',\n",
       "    'absence_wkd_4_gr_6',\n",
       "    'tardy_wkd_4_gr_5',\n",
       "    'absence_wkd_4_gr_5',\n",
       "    'tardy_wkd_2_gr_5',\n",
       "    'tardy_wkd_3_gr_5',\n",
       "    'absence_unexcused_gr_5',\n",
       "    'absence_wkd_3_gr_5',\n",
       "    'absence_wkd_5_gr_5',\n",
       "    'tardy_unexcused_gr_5',\n",
       "    'absence_wkd_1_gr_5',\n",
       "    'absence_wkd_2_gr_5',\n",
       "    'absence_gr_6',\n",
       "    'tardy_wkd_1_gr_6',\n",
       "    'absence_gr_5',\n",
       "    'tardy_wkd_5_gr_5',\n",
       "    'tardy_wkd_5_gr_6',\n",
       "    'absence_consec_gr_5'],\n",
       "   'demographics': ['ethnicity', 'gender'],\n",
       "   'grades': ['future_prep_gpa_gr_5',\n",
       "    'interventions_gpa_gr_6',\n",
       "    'num_language_classes_gr_6',\n",
       "    'num_interventions_classes_gr_6',\n",
       "    'num_stem_classes_gr_6',\n",
       "    'art_gpa_gr_5',\n",
       "    'health_gpa_gr_6',\n",
       "    'art_gpa_gr_6',\n",
       "    'num_humanities_classes_gr_5',\n",
       "    'health_gpa_gr_5',\n",
       "    'num_stem_classes_gr_5',\n",
       "    'humanities_gpa_gr_6',\n",
       "    'num_art_classes_gr_6',\n",
       "    'stem_gpa_gr_6',\n",
       "    'num_future_prep_classes_gr_5',\n",
       "    'interventions_gpa_gr_5',\n",
       "    'gpa_district_gr_6',\n",
       "    'humanities_gpa_gr_5',\n",
       "    'num_humanities_classes_gr_6',\n",
       "    'percent_passed_pf_classes_gr_6',\n",
       "    'num_future_prep_classes_gr_6',\n",
       "    'num_pf_classes_gr_5',\n",
       "    'num_language_classes_gr_5',\n",
       "    'language_gpa_gr_6',\n",
       "    'num_art_classes_gr_5',\n",
       "    'num_health_classes_gr_5',\n",
       "    'language_gpa_gr_5',\n",
       "    'gpa_gr_6',\n",
       "    'percent_passed_pf_classes_gr_5',\n",
       "    'num_pf_classes_gr_6',\n",
       "    'stem_gpa_gr_5',\n",
       "    'gpa_district_gr_5',\n",
       "    'num_interventions_classes_gr_5',\n",
       "    'gpa_gr_5',\n",
       "    'num_health_classes_gr_6',\n",
       "    'future_prep_gpa_gr_6'],\n",
       "   'intervention': ['school_program_gr_5',\n",
       "    'academic_inv_gr_5',\n",
       "    'extracurr_program_gr_5',\n",
       "    'school_program_gr_6',\n",
       "    'spec_instruc_gr_5',\n",
       "    'post_secondary_gr_6',\n",
       "    'post_secondary_gr_5',\n",
       "    'extracurr_program_gr_6',\n",
       "    'placement_gr_5',\n",
       "    'titlei_gr_5',\n",
       "    'academic_inv_gr_6',\n",
       "    'atheletics_gr_6',\n",
       "    'vocational_gr_6',\n",
       "    'academic_intracurr_gr_5',\n",
       "    'vocational_gr_5',\n",
       "    'titlei_gr_6',\n",
       "    'placement_gr_6',\n",
       "    'atheletics_gr_5',\n",
       "    'spec_instruc_gr_6',\n",
       "    'academic_intracurr_gr_6'],\n",
       "   'mobility': ['avg_address_change_to_gr_5',\n",
       "    'avg_address_change_to_gr_6',\n",
       "    'street_transition_in_gr_5',\n",
       "    'street_transition_in_gr_6',\n",
       "    'n_cities_to_gr_6',\n",
       "    'district_transition_in_gr_5',\n",
       "    'mid_year_withdraw_gr_5',\n",
       "    'n_districts_to_gr_6',\n",
       "    'n_districts_to_gr_5',\n",
       "    'n_records_to_gr_6',\n",
       "    'n_records_to_gr_5',\n",
       "    'avg_district_change_to_gr_5',\n",
       "    'n_addresses_to_gr_5',\n",
       "    'city_transition_in_gr_5',\n",
       "    'avg_city_change_to_gr_6',\n",
       "    'avg_city_change_to_gr_5',\n",
       "    'n_cities_to_gr_5',\n",
       "    'city_transition_in_gr_6',\n",
       "    'mid_year_withdraw_gr_6',\n",
       "    'n_addresses_to_gr_6',\n",
       "    'avg_district_change_to_gr_6',\n",
       "    'district_transition_in_gr_6'],\n",
       "   'oaa_normalized': ['read_normalized_gr_5',\n",
       "    'read_normalized_gr_4',\n",
       "    'socstudies_normalized_gr_5',\n",
       "    'read_normalized_gr_3',\n",
       "    'math_normalized_gr_3',\n",
       "    'science_normalized_gr_5',\n",
       "    'math_normalized_gr_4',\n",
       "    'math_normalized_gr_6',\n",
       "    'math_normalized_gr_5',\n",
       "    'read_normalized_gr_6'],\n",
       "   'snapshots': ['iss_gr_6',\n",
       "    'special_ed_gr_6',\n",
       "    'iss_gr_5',\n",
       "    'status_gr_5',\n",
       "    'status_gr_6',\n",
       "    'section_504_plan_gr_5',\n",
       "    'gifted_gr_6',\n",
       "    'disadvantagement_gr_5',\n",
       "    'discipline_incidents_gr_5',\n",
       "    'limited_english_gr_5',\n",
       "    'limited_english_gr_6',\n",
       "    'disadvantagement_gr_6',\n",
       "    'disability_gr_5',\n",
       "    'disability_gr_6',\n",
       "    'special_ed_gr_5',\n",
       "    'district_gr_6',\n",
       "    'section_504_plan_gr_6',\n",
       "    'district_gr_5',\n",
       "    'gifted_gr_5',\n",
       "    'oss_gr_5',\n",
       "    'discipline_incidents_gr_6',\n",
       "    'oss_gr_6']},\n",
       "  'file_save_name': '08_12_2016_grade_7_param_set_17',\n",
       "  'missing_impute_strategy': 'median_plus_dummies',\n",
       "  'model_classes_selected': ['logit', 'DT', 'RF', 'ET', 'SVM'],\n",
       "  'model_test_holdout': 'temporal_cohort',\n",
       "  'n_folds': 5,\n",
       "  'outcome_name': 'definite_plus_ogt',\n",
       "  'parameter_cross_validation_scheme': 'k_fold',\n",
       "  'prediction_grade_level': 7,\n",
       "  'random_seed': 1471079940.7053018,\n",
       "  'sample_wt_ratio': None,\n",
       "  'subset_n': None,\n",
       "  'upsample_param': None,\n",
       "  'user': 'ht',\n",
       "  'user_description': 'third pass for grade 7',\n",
       "  'validation_criterion': ['custom_precision_5_15', 'custom_recall_5_15'],\n",
       "  'write_predictions_to_database': True},\n",
       " 'parameter_grid': {'C': [0.01, 0.1, 1.0, 10.0, 100, 1000],\n",
       "  'penalty': ['l1', 'l2']},\n",
       " 'test_set_preds': array([ 0.,  0.,  0., ...,  1.,  0.,  1.]),\n",
       " 'test_set_soft_preds': array([ 0.20472843,  0.06372438,  0.03285757, ...,  0.98952511,\n",
       "         0.29188668,  0.8015029 ]),\n",
       " 'test_y': student_lookup\n",
       " 12524.0    0.0\n",
       " 269.0      0.0\n",
       " 286.0      0.0\n",
       " 596.0      0.0\n",
       " 622.0      0.0\n",
       " 624.0      0.0\n",
       " 681.0      0.0\n",
       " 766.0      0.0\n",
       " 781.0      0.0\n",
       " 785.0      0.0\n",
       " 786.0      0.0\n",
       " 790.0      0.0\n",
       " 791.0      0.0\n",
       " 798.0      0.0\n",
       " 809.0      0.0\n",
       " 811.0      0.0\n",
       " 812.0      0.0\n",
       " 814.0      0.0\n",
       " 815.0      0.0\n",
       " 816.0      0.0\n",
       " 817.0      0.0\n",
       " 819.0      0.0\n",
       " 820.0      0.0\n",
       " 821.0      0.0\n",
       " 823.0      0.0\n",
       " 824.0      0.0\n",
       " 825.0      0.0\n",
       " 828.0      0.0\n",
       " 833.0      0.0\n",
       " 834.0      0.0\n",
       "           ... \n",
       " 49302.0    1.0\n",
       " 49334.0    1.0\n",
       " 49399.0    1.0\n",
       " 49490.0    1.0\n",
       " 49503.0    1.0\n",
       " 49551.0    1.0\n",
       " 49583.0    1.0\n",
       " 49653.0    1.0\n",
       " 49680.0    1.0\n",
       " 49699.0    1.0\n",
       " 49739.0    1.0\n",
       " 49819.0    1.0\n",
       " 49857.0    1.0\n",
       " 49863.0    1.0\n",
       " 51112.0    1.0\n",
       " 51161.0    1.0\n",
       " 51247.0    1.0\n",
       " 52746.0    1.0\n",
       " 54035.0    1.0\n",
       " 54225.0    1.0\n",
       " 54240.0    1.0\n",
       " 54327.0    1.0\n",
       " 54374.0    1.0\n",
       " 54687.0    1.0\n",
       " 54726.0    1.0\n",
       " 55013.0    1.0\n",
       " 55151.0    1.0\n",
       " 47.0       1.0\n",
       " 49.0       0.0\n",
       " 50.0       0.0\n",
       " Name: definite_plus_ogt, dtype: float64,\n",
       " 'time': 1.179145336151123,\n",
       " 'train_set_balance': {0: 808, 1: 89},\n",
       " 'train_set_preds': array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  1.,\n",
       "         0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,\n",
       "         0.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  1.,  1.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,\n",
       "         0.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,  1.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "         0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  1.,  1.,  1.,  1.,  0.,  1.,  1.,  0.,  0.,  1.,  1.,  1.,\n",
       "         1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  1.,  1.,  0.,\n",
       "         1.,  0.,  0.,  1.,  1.,  1.,  0.,  1.,  0.,  0.,  1.,  0.,  1.,\n",
       "         0.,  0.,  0.,  1.,  0.,  1.,  1.,  1.,  0.,  1.,  0.,  1.,  1.,\n",
       "         0.,  0.,  1.,  0.,  0.,  1.,  0.,  1.,  1.,  1.,  1.,  0.,  1.,\n",
       "         1.,  0.,  1.,  0.,  1.,  0.,  0.,  1.,  1.,  1.,  0.,  1.,  1.]),\n",
       " 'train_set_soft_preds': array([  1.28541462e-01,   6.53367582e-01,   2.29781769e-01,\n",
       "          3.53880516e-01,   1.14114049e-01,   4.68183107e-01,\n",
       "          1.91711552e-01,   1.87749945e-01,   8.34070339e-01,\n",
       "          3.24138456e-01,   6.89244317e-02,   5.50608938e-01,\n",
       "          5.22911654e-01,   3.56291125e-01,   9.87778975e-02,\n",
       "          2.75712387e-01,   5.41989465e-01,   1.55097932e-01,\n",
       "          1.10030996e-01,   2.68593707e-01,   3.75801763e-02,\n",
       "          2.17642434e-02,   9.46107885e-02,   1.13793882e-01,\n",
       "          1.69141940e-02,   4.99621917e-01,   1.20955765e-01,\n",
       "          2.68593206e-02,   9.09515439e-02,   7.43304308e-02,\n",
       "          2.83427986e-01,   9.95180038e-02,   1.90714770e-02,\n",
       "          1.72664429e-01,   7.46166488e-02,   2.03831554e-01,\n",
       "          5.58659868e-02,   4.94955428e-02,   5.80397046e-01,\n",
       "          8.76536539e-02,   6.59328082e-02,   1.82883860e-01,\n",
       "          5.80756479e-02,   2.85917555e-02,   2.30148116e-02,\n",
       "          2.72458707e-01,   1.22258084e-01,   2.36089051e-01,\n",
       "          5.52745438e-02,   4.90012710e-02,   1.46351896e-01,\n",
       "          1.01065027e-01,   1.22051119e-02,   1.05073434e-02,\n",
       "          4.96324353e-02,   1.01689138e-01,   9.53789224e-02,\n",
       "          6.21419658e-02,   7.27808757e-02,   5.83132450e-02,\n",
       "          1.36136079e-01,   7.29352172e-02,   3.35173281e-02,\n",
       "          1.03166121e-01,   6.31394868e-02,   4.64663903e-02,\n",
       "          2.37320508e-01,   1.98269241e-02,   6.48507701e-02,\n",
       "          1.51239022e-02,   9.46037886e-02,   5.16605058e-02,\n",
       "          2.64288046e-02,   2.36327456e-02,   4.86414478e-02,\n",
       "          2.78070188e-01,   3.53806464e-02,   3.53471447e-02,\n",
       "          6.52277320e-02,   7.22515940e-02,   3.33138454e-01,\n",
       "          2.54792294e-01,   2.93076507e-02,   5.22480709e-03,\n",
       "          1.25850546e-01,   1.59395770e-01,   7.28499685e-02,\n",
       "          4.79834486e-02,   1.52055277e-01,   3.07422996e-02,\n",
       "          2.44383213e-01,   1.15381974e-01,   3.00310513e-01,\n",
       "          8.36978048e-02,   2.44901017e-01,   3.94175931e-02,\n",
       "          3.33967674e-02,   6.99475690e-02,   1.55777567e-02,\n",
       "          6.28125170e-02,   1.42911539e-01,   3.28671836e-02,\n",
       "          9.97712819e-02,   4.47267167e-01,   1.53128067e-01,\n",
       "          7.76980472e-01,   9.84212520e-02,   3.93465424e-02,\n",
       "          1.52445282e-01,   9.57812234e-02,   1.87878055e-02,\n",
       "          2.26022923e-01,   8.53160309e-02,   6.18154105e-02,\n",
       "          7.15688729e-02,   3.71845283e-01,   9.43358011e-02,\n",
       "          1.50894819e-01,   1.39648153e-01,   3.24826911e-02,\n",
       "          1.81325319e-02,   5.39204749e-02,   5.00635352e-02,\n",
       "          1.11211929e-01,   3.06103945e-02,   8.29968327e-02,\n",
       "          1.96539627e-02,   5.37269574e-02,   1.07882758e-01,\n",
       "          1.48338156e-01,   2.33079869e-02,   3.84693248e-02,\n",
       "          1.88717780e-01,   2.88784943e-02,   5.77511894e-02,\n",
       "          1.84945473e-01,   1.16070811e-01,   2.88911211e-01,\n",
       "          4.91354403e-02,   2.11883668e-02,   9.97575263e-02,\n",
       "          4.70618366e-02,   8.07844331e-03,   5.20528018e-01,\n",
       "          4.18855660e-02,   7.69373910e-02,   7.60713839e-02,\n",
       "          3.02997673e-01,   3.71778289e-02,   1.67976245e-02,\n",
       "          2.54510856e-02,   4.00742007e-01,   1.03047156e-01,\n",
       "          6.53717729e-02,   2.50172518e-02,   3.33926537e-02,\n",
       "          1.28835345e-02,   5.70872966e-03,   4.04873437e-03,\n",
       "          4.65561383e-01,   6.57753473e-01,   1.76300422e-01,\n",
       "          1.30845224e-01,   1.72618677e-02,   3.66796669e-01,\n",
       "          3.51545882e-03,   1.12363747e-01,   3.79608520e-02,\n",
       "          7.87850538e-01,   9.19494483e-02,   4.06948876e-01,\n",
       "          3.24180780e-02,   3.88201328e-02,   1.12187517e-03,\n",
       "          3.55204795e-02,   2.13761100e-02,   1.29052824e-02,\n",
       "          5.68047932e-03,   1.79274024e-01,   4.17547253e-01,\n",
       "          2.93042999e-02,   3.35713938e-01,   5.68044736e-02,\n",
       "          1.48207690e-01,   3.63762626e-02,   2.96033886e-01,\n",
       "          2.97241569e-01,   8.59776181e-03,   9.43399009e-02,\n",
       "          2.04700235e-01,   8.79719535e-02,   1.27563968e-01,\n",
       "          2.85968177e-01,   5.92652366e-02,   3.46201628e-01,\n",
       "          1.70084679e-02,   9.45291806e-03,   4.50117695e-02,\n",
       "          2.61207172e-02,   2.92479032e-01,   2.56307820e-01,\n",
       "          2.17487096e-02,   3.07785247e-01,   6.14786633e-01,\n",
       "          6.62482692e-01,   8.39262531e-03,   4.44910252e-02,\n",
       "          1.69117758e-02,   6.57888625e-02,   7.20265664e-01,\n",
       "          6.12012236e-01,   7.49851052e-01,   2.76348438e-01,\n",
       "          4.96222353e-02,   8.20579107e-02,   9.05803784e-03,\n",
       "          4.28121356e-02,   6.87387745e-02,   4.81584841e-02,\n",
       "          7.97783778e-03,   1.74682016e-01,   8.24594165e-02,\n",
       "          9.38561160e-02,   2.67027736e-02,   2.16354642e-01,\n",
       "          1.11388516e-01,   2.23022376e-02,   5.62291871e-02,\n",
       "          4.13214394e-02,   3.74722005e-02,   1.47667512e-02,\n",
       "          1.02956861e-01,   5.42026301e-02,   3.03844910e-02,\n",
       "          2.31721325e-01,   9.00218242e-02,   6.46959155e-02,\n",
       "          1.94590900e-02,   1.98314982e-02,   4.28999725e-02,\n",
       "          1.08694681e-02,   3.53373689e-03,   1.33489873e-01,\n",
       "          5.63708044e-02,   1.31711001e-02,   1.76419289e-02,\n",
       "          1.72445749e-01,   1.18822935e-02,   3.61284505e-02,\n",
       "          1.38805281e-02,   3.11750009e-01,   3.90731477e-01,\n",
       "          6.56786449e-01,   3.94307946e-02,   7.68065597e-02,\n",
       "          5.08842376e-02,   1.35552644e-01,   5.25868163e-02,\n",
       "          4.62996511e-02,   2.29580736e-02,   6.18055106e-02,\n",
       "          7.26907625e-03,   2.23172064e-01,   2.72078991e-01,\n",
       "          7.06880604e-01,   1.38343794e-01,   6.25510715e-02,\n",
       "          6.71108761e-02,   5.14520885e-02,   1.79495384e-01,\n",
       "          1.57070487e-01,   1.68368564e-01,   1.48099558e-01,\n",
       "          1.22008314e-02,   3.50237925e-01,   4.48129778e-02,\n",
       "          2.41254740e-02,   8.08295992e-02,   4.80608930e-02,\n",
       "          5.17353446e-01,   2.85787882e-01,   2.24140279e-01,\n",
       "          2.30743949e-01,   2.35968465e-01,   1.26205927e-02,\n",
       "          3.67882101e-02,   4.77526914e-01,   4.31011461e-02,\n",
       "          1.15789366e-02,   1.36014557e-02,   5.34740371e-02,\n",
       "          9.02031244e-01,   2.07164983e-01,   6.69018161e-01,\n",
       "          7.11558336e-03,   1.33817551e-02,   8.51466021e-02,\n",
       "          5.45479279e-01,   5.31798528e-01,   4.61092721e-03,\n",
       "          1.58584311e-02,   1.35867321e-02,   1.34728600e-02,\n",
       "          4.80563288e-02,   1.71655440e-02,   4.41932828e-03,\n",
       "          3.74373339e-02,   2.62525686e-03,   5.22683152e-02,\n",
       "          5.58841215e-03,   2.19895514e-02,   6.68120361e-02,\n",
       "          1.33695876e-01,   1.37897110e-01,   6.85038338e-02,\n",
       "          4.78013444e-01,   1.29477941e-01,   2.26059164e-01,\n",
       "          5.81983154e-01,   9.63982433e-02,   2.09779954e-01,\n",
       "          5.04227699e-01,   3.51795009e-01,   9.49708162e-02,\n",
       "          1.72772223e-02,   7.26895083e-03,   1.02565395e-01,\n",
       "          7.09065248e-02,   2.88338443e-01,   2.87150482e-03,\n",
       "          4.28419499e-01,   3.73167205e-02,   3.60327926e-02,\n",
       "          6.71378327e-02,   1.29152057e-01,   2.62389449e-03,\n",
       "          6.20794030e-03,   2.95381781e-03,   1.09908665e-03,\n",
       "          6.06522405e-02,   7.99822025e-02,   1.69710636e-02,\n",
       "          6.83391540e-03,   2.02277764e-03,   7.52304087e-03,\n",
       "          3.51763286e-01,   8.43938773e-03,   8.47015847e-03,\n",
       "          1.32644677e-01,   3.31296402e-03,   4.41268073e-03,\n",
       "          3.70312924e-02,   1.34080039e-03,   1.59334222e-03,\n",
       "          2.80005474e-03,   1.36664120e-02,   2.13350092e-03,\n",
       "          2.05421460e-03,   1.63648080e-01,   6.50189477e-03,\n",
       "          3.83591330e-01,   1.08904738e-03,   2.20632499e-02,\n",
       "          1.23299945e-01,   4.01482211e-03,   8.27048926e-02,\n",
       "          5.39414705e-03,   6.65803037e-02,   1.38048016e-01,\n",
       "          3.01356665e-02,   1.46859675e-01,   4.01133155e-02,\n",
       "          6.20237980e-02,   4.29126722e-01,   1.11979260e-01,\n",
       "          4.34892912e-02,   4.50036311e-02,   3.91340613e-01,\n",
       "          2.06826247e-01,   3.27054683e-02,   4.32222607e-01,\n",
       "          6.33036569e-01,   3.82560995e-01,   2.66542326e-01,\n",
       "          8.85273145e-02,   1.09965773e-01,   1.22447291e-01,\n",
       "          3.66376890e-01,   1.26099733e-01,   4.66965008e-01,\n",
       "          1.68016279e-01,   1.28049314e-01,   8.50977691e-03,\n",
       "          2.79101353e-01,   4.14180017e-01,   3.21606858e-02,\n",
       "          5.74144166e-02,   2.01357230e-02,   6.87568077e-02,\n",
       "          2.61351082e-02,   2.36557623e-01,   9.28423041e-02,\n",
       "          6.44798310e-02,   8.77015927e-02,   6.02765811e-02,\n",
       "          1.00630305e-01,   3.45552709e-02,   5.99769306e-02,\n",
       "          1.32195128e-01,   2.81024624e-02,   3.87916860e-02,\n",
       "          2.30703237e-01,   1.64711700e-02,   4.56819771e-01,\n",
       "          1.13188418e-02,   3.57351165e-02,   6.99776379e-02,\n",
       "          4.91481296e-02,   8.59203414e-03,   1.93616509e-01,\n",
       "          5.55079591e-03,   4.54288546e-02,   4.42060965e-03,\n",
       "          2.52114698e-01,   4.98421688e-03,   1.47457849e-01,\n",
       "          2.22515421e-01,   9.78247379e-02,   4.51501695e-01,\n",
       "          5.37475777e-02,   3.29843726e-02,   3.32748603e-02,\n",
       "          4.57396946e-02,   3.63429682e-01,   2.57786763e-02,\n",
       "          2.05685478e-03,   2.59329598e-01,   2.81041273e-02,\n",
       "          5.37836493e-02,   4.83693299e-02,   5.30907002e-02,\n",
       "          1.15284300e-02,   9.35735254e-03,   1.12010972e-02,\n",
       "          7.11365770e-02,   2.94102021e-02,   2.08516181e-01,\n",
       "          7.02425995e-01,   2.29788392e-02,   2.51577848e-02,\n",
       "          3.74784195e-02,   8.84282677e-02,   7.06369007e-02,\n",
       "          6.26149278e-03,   4.12445913e-03,   1.50079395e-03,\n",
       "          1.36328796e-03,   4.00652798e-01,   2.90794561e-03,\n",
       "          4.09501827e-02,   5.30980355e-04,   9.35811929e-03,\n",
       "          1.64197619e-02,   6.28880218e-02,   4.44824741e-03,\n",
       "          3.93379325e-03,   9.48428241e-03,   2.77479616e-02,\n",
       "          9.70414238e-03,   1.35456777e-02,   5.57316604e-03,\n",
       "          4.68946144e-02,   3.05727752e-03,   2.31395095e-03,\n",
       "          1.06447146e-01,   2.71840241e-02,   6.44993551e-01,\n",
       "          4.38109929e-02,   6.80201252e-03,   2.05786041e-02,\n",
       "          1.93643105e-01,   1.95729221e-02,   2.00885503e-03,\n",
       "          5.21332997e-02,   2.15615163e-03,   1.93122155e-02,\n",
       "          1.88327724e-03,   5.78641255e-03,   2.95569062e-03,\n",
       "          1.60886698e-02,   2.04113215e-02,   4.76799202e-01,\n",
       "          6.23122252e-02,   2.49495180e-02,   1.82939121e-01,\n",
       "          1.13153026e-01,   1.86497694e-02,   5.08939386e-03,\n",
       "          4.26104932e-02,   1.64320801e-01,   1.57525664e-01,\n",
       "          1.58406585e-01,   7.49821619e-02,   1.63503612e-02,\n",
       "          5.11403348e-02,   6.49020128e-01,   7.17535742e-03,\n",
       "          3.15403733e-01,   2.81684044e-01,   5.52649630e-01,\n",
       "          1.84414224e-01,   2.49363461e-01,   2.09695726e-02,\n",
       "          2.83900588e-01,   7.71750315e-01,   2.07518763e-02,\n",
       "          1.93033341e-02,   8.24541457e-02,   1.95729269e-02,\n",
       "          1.20816564e-02,   1.77206614e-01,   1.56440628e-01,\n",
       "          7.14974389e-02,   2.32097453e-02,   1.85904040e-02,\n",
       "          1.89420193e-01,   5.26360814e-02,   8.66714946e-02,\n",
       "          4.68180970e-01,   4.97987397e-02,   2.25980682e-01,\n",
       "          1.22671206e-01,   3.36740399e-01,   1.72533231e-02,\n",
       "          3.53983977e-01,   4.03728239e-02,   3.56614527e-01,\n",
       "          2.13139926e-01,   4.56040403e-01,   3.34779493e-02,\n",
       "          6.38183918e-02,   2.55432387e-02,   1.89648289e-01,\n",
       "          1.83001289e-03,   4.42875693e-02,   6.90824931e-02,\n",
       "          9.15140049e-02,   1.01877595e-02,   7.00452933e-01,\n",
       "          7.21368229e-02,   4.52018608e-02,   2.80251327e-01,\n",
       "          1.35345470e-01,   3.58873615e-02,   9.01828576e-03,\n",
       "          5.12104904e-02,   1.08535904e-02,   7.77080826e-02,\n",
       "          1.02134372e-02,   3.69872333e-03,   7.32490051e-03,\n",
       "          6.75549834e-02,   5.99097339e-02,   3.51294647e-01,\n",
       "          2.54421892e-02,   1.55040195e-01,   3.53521096e-03,\n",
       "          2.64720435e-01,   6.28363098e-03,   2.90213290e-02,\n",
       "          9.84156980e-02,   4.93592316e-03,   1.84150913e-03,\n",
       "          1.72220833e-03,   1.25802129e-03,   2.72650504e-01,\n",
       "          1.51459458e-02,   3.89675087e-01,   3.40342496e-01,\n",
       "          1.11106616e-02,   6.48326863e-02,   2.76348438e-01,\n",
       "          4.66870164e-01,   5.88694850e-01,   1.27727713e-01,\n",
       "          2.20101777e-02,   2.32066631e-02,   3.29742763e-01,\n",
       "          3.32885303e-01,   8.53213831e-02,   1.73561508e-02,\n",
       "          8.55265338e-02,   6.32162546e-01,   1.11991781e-02,\n",
       "          1.47178463e-01,   9.29129376e-03,   2.32662765e-01,\n",
       "          1.43440747e-01,   1.87092610e-02,   1.34409371e-02,\n",
       "          2.69025047e-02,   2.79412909e-02,   9.69937959e-03,\n",
       "          5.60348569e-02,   1.58000960e-01,   1.07981563e-02,\n",
       "          3.89730923e-01,   2.24140279e-01,   5.06974703e-01,\n",
       "          2.10859839e-01,   2.42304892e-01,   1.08945740e-01,\n",
       "          9.21382323e-03,   1.48642833e-01,   5.61338030e-02,\n",
       "          2.08488545e-02,   1.41507994e-01,   6.58660229e-02,\n",
       "          2.24140279e-01,   4.22499360e-02,   5.24625727e-01,\n",
       "          3.05741368e-02,   2.19769755e-01,   1.60366164e-01,\n",
       "          3.84060444e-02,   1.38677977e-02,   5.19841686e-02,\n",
       "          1.36747926e-01,   1.42060044e-02,   5.00433916e-02,\n",
       "          2.70538790e-01,   4.19994527e-01,   2.73088824e-02,\n",
       "          2.76348438e-01,   5.38588807e-02,   2.05238829e-01,\n",
       "          1.45741401e-02,   8.90566594e-02,   5.98423756e-01,\n",
       "          1.70845488e-01,   9.55367000e-02,   7.55734881e-02,\n",
       "          2.92439317e-01,   2.68638111e-03,   1.38893756e-02,\n",
       "          5.75313596e-02,   1.92564558e-02,   1.46982219e-02,\n",
       "          6.95259812e-02,   4.84147910e-01,   6.47584267e-02,\n",
       "          8.95451259e-02,   2.98987625e-01,   1.30670987e-02,\n",
       "          6.95364448e-02,   1.00143333e-01,   1.03337817e-02,\n",
       "          7.51663053e-02,   9.91798288e-02,   1.71271773e-01,\n",
       "          2.36842991e-02,   1.92648229e-01,   6.05122078e-02,\n",
       "          1.50752425e-01,   3.25684459e-01,   6.34844261e-02,\n",
       "          2.73576762e-02,   1.15744348e-01,   9.30345295e-02,\n",
       "          3.06216222e-01,   7.62588786e-02,   1.85384622e-01,\n",
       "          6.21592392e-01,   6.03741408e-02,   3.65417990e-01,\n",
       "          5.91899176e-01,   2.55862623e-01,   1.29332366e-01,\n",
       "          5.39333497e-02,   4.86891970e-02,   1.24122788e-01,\n",
       "          8.25937641e-03,   5.68733175e-01,   1.63499904e-01,\n",
       "          5.17161301e-01,   5.20588569e-01,   1.31388556e-01,\n",
       "          3.24321122e-01,   9.40766987e-02,   5.03916244e-02,\n",
       "          8.74089781e-02,   1.76254522e-02,   9.49493907e-02,\n",
       "          4.07158549e-01,   1.13211969e-01,   3.40189464e-01,\n",
       "          2.83998295e-01,   1.48611894e-01,   3.28398054e-02,\n",
       "          6.52593597e-02,   1.56370300e-01,   3.71644093e-01,\n",
       "          2.10859839e-01,   1.32053124e-01,   5.38191652e-02,\n",
       "          2.86812363e-01,   6.48368302e-03,   1.66040728e-01,\n",
       "          2.35473969e-02,   6.29289235e-02,   6.33423646e-03,\n",
       "          4.33721749e-02,   7.52577511e-03,   1.31409274e-01,\n",
       "          5.79877040e-03,   3.69054730e-02,   6.64775626e-03,\n",
       "          3.19191005e-02,   2.69912338e-02,   6.17792281e-04,\n",
       "          1.11679750e-01,   2.38384867e-02,   1.29085569e-01,\n",
       "          5.11412633e-02,   9.46909295e-03,   1.71875825e-02,\n",
       "          5.65362184e-02,   4.94515304e-04,   2.44623941e-02,\n",
       "          1.10296214e-02,   1.93942506e-02,   3.95768335e-02,\n",
       "          3.05396717e-01,   5.70124907e-02,   3.45454910e-03,\n",
       "          7.13223616e-03,   7.56487598e-03,   1.31464947e-02,\n",
       "          2.24140279e-01,   2.24140279e-01,   2.76348438e-01,\n",
       "          2.19290546e-01,   6.68589910e-01,   1.60332860e-01,\n",
       "          6.57442754e-02,   2.24140279e-01,   6.04144028e-01,\n",
       "          2.24140279e-01,   1.93242470e-01,   7.09027128e-03,\n",
       "          2.02755693e-01,   2.95347645e-02,   3.18098748e-02,\n",
       "          7.29025743e-01,   1.34617429e-02,   8.92355940e-02,\n",
       "          3.22844937e-01,   7.59801570e-02,   7.40905139e-01,\n",
       "          2.88952120e-02,   4.06967416e-02,   4.16780940e-02,\n",
       "          3.96569749e-02,   8.88002279e-02,   4.16995320e-02,\n",
       "          9.32142250e-02,   5.99238312e-01,   6.70300684e-02,\n",
       "          2.08574069e-01,   1.17147642e-01,   4.89484531e-02,\n",
       "          5.39603073e-02,   1.88610953e-01,   4.38630313e-01,\n",
       "          2.09486609e-02,   5.75555596e-02,   2.62324582e-02,\n",
       "          4.57330860e-02,   1.12447074e-01,   1.07323619e-01,\n",
       "          4.27321711e-02,   1.20942043e-01,   8.59852301e-02,\n",
       "          6.90359196e-03,   3.28570071e-02,   1.13212134e-01,\n",
       "          2.94622181e-02,   1.52680513e-02,   3.44987135e-02,\n",
       "          2.68022297e-01,   4.69557237e-02,   7.54284519e-02,\n",
       "          4.98632013e-02,   1.26298607e-02,   9.72474764e-02,\n",
       "          3.71472546e-01,   2.41230913e-01,   1.31658347e-01,\n",
       "          8.76161927e-02,   9.73030759e-03,   4.16052195e-02,\n",
       "          2.22851155e-01,   6.47747177e-01,   2.74650165e-01,\n",
       "          3.76062687e-02,   3.63255233e-01,   3.52736741e-01,\n",
       "          3.72179967e-01,   4.04112251e-02,   9.87807167e-01,\n",
       "          2.68544670e-01,   4.39867417e-01,   4.57464652e-01,\n",
       "          5.41081511e-01,   1.32262085e-01,   6.31398907e-02,\n",
       "          4.68211993e-01,   6.08594997e-02,   2.70748466e-01,\n",
       "          5.69019127e-02,   5.79814078e-01,   8.90507580e-01,\n",
       "          6.11851661e-01,   8.60043732e-01,   4.83243401e-01,\n",
       "          5.74464477e-01,   7.94972715e-01,   4.99590683e-01,\n",
       "          4.25702533e-01,   5.24526874e-01,   8.69175349e-01,\n",
       "          6.04144028e-01,   7.43300724e-01,   4.90029528e-01,\n",
       "          1.70456583e-01,   1.51176062e-01,   1.41962713e-01,\n",
       "          3.83251502e-01,   8.02316812e-01,   1.19360193e-01,\n",
       "          1.73969326e-01,   6.21633540e-01,   8.02464336e-01,\n",
       "          5.93218225e-01,   1.35525353e-01,   5.47683666e-01,\n",
       "          2.34365546e-01,   4.93082709e-01,   9.67138374e-01,\n",
       "          6.68410172e-01,   5.44893113e-01,   3.60945911e-01,\n",
       "          9.13784963e-01,   1.53521457e-01,   2.03584707e-01,\n",
       "          7.33718360e-01,   3.38089789e-01,   8.33433887e-01,\n",
       "          2.97306168e-01,   1.24558056e-01,   6.81382110e-02,\n",
       "          5.85903963e-01,   3.97977138e-01,   8.13937017e-01,\n",
       "          8.55099153e-01,   6.72040542e-01,   3.03620575e-01,\n",
       "          6.12481953e-01,   1.43097504e-01,   7.23824260e-01,\n",
       "          5.15528823e-01,   3.46616101e-01,   3.01084113e-01,\n",
       "          7.25438261e-01,   1.15832725e-01,   3.53526002e-01,\n",
       "          6.75877797e-01,   4.75928087e-01,   7.24368303e-01,\n",
       "          6.15747773e-01,   6.09890587e-01,   8.57203369e-01,\n",
       "          3.93868754e-01,   5.28905203e-01,   5.36287442e-01,\n",
       "          3.71139521e-01,   9.78422210e-01,   4.47089054e-01,\n",
       "          6.80759612e-01,   2.72606153e-01,   2.18347476e-01,\n",
       "          6.43162869e-01,   7.04012986e-01,   7.53632927e-01,\n",
       "          1.97352151e-01,   9.02850944e-01,   8.23495506e-01]),\n",
       " 'train_y': student_lookup\n",
       " 42053.0    0.0\n",
       " 1513.0     1.0\n",
       " 2690.0     1.0\n",
       " 5552.0     1.0\n",
       " 5778.0     0.0\n",
       " 14335.0    1.0\n",
       " 16680.0    0.0\n",
       " 34375.0    0.0\n",
       " 34376.0    0.0\n",
       " 34377.0    0.0\n",
       " 34378.0    0.0\n",
       " 34379.0    0.0\n",
       " 34380.0    0.0\n",
       " 34381.0    0.0\n",
       " 34382.0    0.0\n",
       " 34383.0    0.0\n",
       " 34388.0    0.0\n",
       " 34405.0    0.0\n",
       " 34412.0    0.0\n",
       " 34413.0    0.0\n",
       " 34419.0    0.0\n",
       " 34425.0    0.0\n",
       " 34428.0    0.0\n",
       " 34430.0    0.0\n",
       " 34431.0    0.0\n",
       " 34468.0    0.0\n",
       " 34470.0    0.0\n",
       " 34520.0    0.0\n",
       " 34521.0    0.0\n",
       " 34522.0    0.0\n",
       "           ... \n",
       " 51180.0    1.0\n",
       " 51186.0    1.0\n",
       " 51199.0    1.0\n",
       " 51225.0    1.0\n",
       " 51240.0    1.0\n",
       " 51264.0    1.0\n",
       " 51269.0    1.0\n",
       " 51273.0    1.0\n",
       " 51344.0    1.0\n",
       " 51356.0    1.0\n",
       " 51417.0    1.0\n",
       " 51421.0    1.0\n",
       " 51470.0    1.0\n",
       " 51474.0    1.0\n",
       " 51486.0    1.0\n",
       " 51505.0    1.0\n",
       " 51507.0    1.0\n",
       " 51509.0    1.0\n",
       " 51515.0    1.0\n",
       " 51546.0    1.0\n",
       " 51567.0    1.0\n",
       " 51624.0    1.0\n",
       " 51636.0    1.0\n",
       " 51700.0    1.0\n",
       " 52266.0    0.0\n",
       " 52277.0    1.0\n",
       " 52322.0    1.0\n",
       " 54553.0    1.0\n",
       " 54979.0    1.0\n",
       " 59631.0    1.0\n",
       " Name: definite_plus_ogt, dtype: float64,\n",
       " 'val_set_preds': array([ 0.,  0.,  0., ...,  1.,  0.,  1.]),\n",
       " 'val_set_soft_preds': array([ 0.40882436,  0.31410029,  0.18258225, ...,  0.65585643,\n",
       "         0.10539598,  0.54565253]),\n",
       " 'val_y': student_lookup\n",
       " 297.0      1.0\n",
       " 734.0      0.0\n",
       " 1161.0     0.0\n",
       " 1531.0     0.0\n",
       " 2486.0     0.0\n",
       " 2489.0     1.0\n",
       " 2692.0     1.0\n",
       " 3138.0     1.0\n",
       " 3139.0     0.0\n",
       " 3140.0     1.0\n",
       " 3684.0     1.0\n",
       " 3686.0     1.0\n",
       " 3687.0     1.0\n",
       " 4992.0     0.0\n",
       " 7063.0     0.0\n",
       " 7570.0     1.0\n",
       " 9980.0     1.0\n",
       " 11028.0    1.0\n",
       " 11259.0    1.0\n",
       " 12044.0    0.0\n",
       " 12045.0    0.0\n",
       " 12300.0    0.0\n",
       " 13404.0    1.0\n",
       " 14446.0    1.0\n",
       " 14926.0    1.0\n",
       " 16723.0    0.0\n",
       " 16767.0    1.0\n",
       " 16774.0    1.0\n",
       " 35718.0    0.0\n",
       " 35753.0    0.0\n",
       "           ... \n",
       " 50342.0    1.0\n",
       " 50415.0    1.0\n",
       " 50449.0    1.0\n",
       " 50454.0    1.0\n",
       " 50455.0    1.0\n",
       " 50462.0    1.0\n",
       " 50477.0    1.0\n",
       " 50510.0    1.0\n",
       " 50514.0    1.0\n",
       " 50581.0    1.0\n",
       " 50597.0    1.0\n",
       " 50600.0    1.0\n",
       " 50624.0    1.0\n",
       " 50655.0    1.0\n",
       " 50690.0    1.0\n",
       " 50706.0    1.0\n",
       " 50726.0    1.0\n",
       " 51047.0    1.0\n",
       " 51284.0    1.0\n",
       " 51293.0    1.0\n",
       " 51412.0    1.0\n",
       " 51569.0    1.0\n",
       " 51671.0    1.0\n",
       " 52556.0    1.0\n",
       " 52645.0    1.0\n",
       " 54164.0    1.0\n",
       " 58051.0    1.0\n",
       " 58496.0    1.0\n",
       " 46.0       1.0\n",
       " 48.0       1.0\n",
       " Name: definite_plus_ogt, dtype: float64}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 58.8 ms\n"
     ]
    }
   ],
   "source": [
    "pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['test_y', 'model_name', 'val_set_preds', 'val_set_soft_preds', 'train_set_preds', 'train_y', 'train_set_soft_preds', 'test_set_preds', 'features', 'test_set_soft_preds', 'model_options', 'estimator', 'cross_validation_scores', 'time', 'parameter_grid', 'train_set_balance', 'file_name', 'val_y'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.29 ms\n"
     ]
    }
   ],
   "source": [
    "pkl.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.37 s\n"
     ]
    }
   ],
   "source": [
    "# it seems the robusted-scaled are centered to median and rescaled by IQR;\n",
    "# we may assume all features has a simiar scales\n",
    "\n",
    "\"\"\"\n",
    "Procedures to generate individual scores and top risk factors of logistic regression\n",
    "1. load the features of current students at a certain grade;\n",
    "2. load the corresonding model pickle file;\n",
    "3. make predictions to get raw score (0, 1)\n",
    "4. rank the scores and assiged risk level and risk score (5% high risk, 5% medium, 5% low; rescale 20% to 1 to 10);\n",
    "5. find the top risk factors/features/column names;\n",
    "6. save as pdf output to csv\n",
    "\"\"\"\n",
    "schema, table = 'model', 'individual_risk_scores_factors'\n",
    "dir_pkls = '/mnt/data/mvesc/Models_Results/pkls'\n",
    "pickle_file = '08_12_2016_grade_7_param_set_17_logit_ht_19082_logit.pkl'\n",
    "if_exists = 'replace'\n",
    "random_seed = 62571\n",
    "topK = 3\n",
    "both_positive_negative = True\n",
    "num_students = 20\n",
    "student_column = 'student_lookup'\n",
    "pkl_model_key = 'estimator'\n",
    "\n",
    "def topK_features_logit(model, data, feature_name, topN=3):\n",
    "    importances = np.transpose(model.coef_)[:, 0]*data\n",
    "    indices = importances.argsort()\n",
    "    indices = indices[::-1]\n",
    "    #print(indices[:3])\n",
    "    return(list(np.array(features)[indices[:3]]))\n",
    "\n",
    "with open(os.path.join(dir_pkls, pickle_file), 'rb') as handle:\n",
    "    pkl = pickle.load(handle)\n",
    "features = list(pkl['features']) # to pull feature data later\n",
    "\n",
    "###!!! generate random all_features_dataframe with student_lookups for testing \n",
    "np.random.seed(random_seed)\n",
    "all_data4prediction = np.random.rand(num_students, len(features))\n",
    "all_data4prediction = pd.DataFrame(all_data4prediction, columns=features)\n",
    "all_data4prediction[student_column] = range(1, all_data4prediction.shape[0]+1)\n",
    "all_data4prediction = all_data4prediction[[student_column]+features]\n",
    "###!!! random all_features_dataframe generated\n",
    "\n",
    "student_lookups = all_data4prediction[student_column]\n",
    "all_feature_data = all_data4prediction[features]\n",
    "# all_feature_data = Robust_Scale(all_feature_data) #processed\n",
    "risk_probas = pkl[pkl_model_key].predict_proba(all_feature_data)[:,1]\n",
    "predictions = pkl[pkl_model_key].predict(all_feature_data)\n",
    "top_individual_features = []\n",
    "for i in range(all_feature_data.shape[0]):\n",
    "    x = np.array(all_feature_data.iloc[i, :])\n",
    "    top_individual_features.append(topN_features_logit(pkl[pkl_model_key], x, features, topN=3))\n",
    "\n",
    "top_risk_factor_names = ['risk_factor_'+str(i) for i in range(1, topK+1)]\n",
    "top_individual_features = pd.DataFrame(top_individual_features, columns=top_risk_factor_names)\n",
    "\n",
    "# individual risk score & factors\n",
    "individual_scores_factors = pd.DataFrame()\n",
    "individual_scores_factors[student_column] = student_lookups\n",
    "individual_scores_factors['risk_score'] =  risk_probas\n",
    "individual_scores_factors = pd.concat([individual_scores_factors, top_individual_features], axis=1)\n",
    "\n",
    "# individual risk facotrs values\n",
    "top_feature_values = {'risk_factor_'+str(i):[] for i in range(1, topK+1)}\n",
    "for risk_i in top_feature_values:\n",
    "    for student_i in range(all_data4prediction.shape[0]):\n",
    "        column_in_alldata = individual_scores_factors.ix[student_i, risk_i]\n",
    "        top_feature_values[risk_i].append(all_data4prediction.ix[student_i, column_in_alldata])\n",
    "top_feature_values = pd.DataFrame(top_feature_values)\n",
    "top_feature_values = top_feature_values.rename(columns={x:x+'_value' for x in top_feature_values.columns})\n",
    "individual_scores_factors = pd.concat([individual_scores_factors, top_feature_values], axis=1)\n",
    "\n",
    "# model and its file name\n",
    "individual_scores_factors['model'] = str(pkl[pkl_model_key])\n",
    "individual_scores_factors['model_file'] = pickle_file\n",
    "\n",
    "eng = postgres_engine_generator()\n",
    "individual_scores_factors.to_sql(table, eng, schema = schema, if_exists=if_exists, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>student_lookup</th>\n",
       "      <th>risk_score</th>\n",
       "      <th>risk_factor_1</th>\n",
       "      <th>risk_factor_2</th>\n",
       "      <th>risk_factor_3</th>\n",
       "      <th>risk_factor_1_value</th>\n",
       "      <th>risk_factor_2_value</th>\n",
       "      <th>risk_factor_3_value</th>\n",
       "      <th>model</th>\n",
       "      <th>model_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.758139</td>\n",
       "      <td>ethnicity_M</td>\n",
       "      <td>district_gr_6_Maysville</td>\n",
       "      <td>read_normalized_gr_5_isnull</td>\n",
       "      <td>0.722477</td>\n",
       "      <td>0.848921</td>\n",
       "      <td>0.876151</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>08_12_2016_grade_7_param_set_17_logit_ht_19082...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.213100</td>\n",
       "      <td>ethnicity_M</td>\n",
       "      <td>disadvantagement_gr_6_economic</td>\n",
       "      <td>art_gpa_gr_6</td>\n",
       "      <td>0.416974</td>\n",
       "      <td>0.965883</td>\n",
       "      <td>0.760293</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>08_12_2016_grade_7_param_set_17_logit_ht_19082...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.493108</td>\n",
       "      <td>ethnicity_M</td>\n",
       "      <td>art_gpa_gr_6</td>\n",
       "      <td>disadvantagement_gr_6_economic</td>\n",
       "      <td>0.654893</td>\n",
       "      <td>0.943058</td>\n",
       "      <td>0.789732</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>08_12_2016_grade_7_param_set_17_logit_ht_19082...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.516004</td>\n",
       "      <td>ethnicity_M</td>\n",
       "      <td>district_gr_6_Maysville</td>\n",
       "      <td>read_normalized_gr_5_isnull</td>\n",
       "      <td>0.878060</td>\n",
       "      <td>0.996202</td>\n",
       "      <td>0.766150</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>08_12_2016_grade_7_param_set_17_logit_ht_19082...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.792288</td>\n",
       "      <td>ethnicity_M</td>\n",
       "      <td>district_gr_6_Maysville</td>\n",
       "      <td>art_gpa_gr_6</td>\n",
       "      <td>0.900475</td>\n",
       "      <td>0.863978</td>\n",
       "      <td>0.803051</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>08_12_2016_grade_7_param_set_17_logit_ht_19082...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   student_lookup  risk_score risk_factor_1                   risk_factor_2  \\\n",
       "0               1    0.758139   ethnicity_M         district_gr_6_Maysville   \n",
       "1               2    0.213100   ethnicity_M  disadvantagement_gr_6_economic   \n",
       "2               3    0.493108   ethnicity_M                    art_gpa_gr_6   \n",
       "3               4    0.516004   ethnicity_M         district_gr_6_Maysville   \n",
       "4               5    0.792288   ethnicity_M         district_gr_6_Maysville   \n",
       "\n",
       "                    risk_factor_3  risk_factor_1_value  risk_factor_2_value  \\\n",
       "0     read_normalized_gr_5_isnull             0.722477             0.848921   \n",
       "1                    art_gpa_gr_6             0.416974             0.965883   \n",
       "2  disadvantagement_gr_6_economic             0.654893             0.943058   \n",
       "3     read_normalized_gr_5_isnull             0.878060             0.996202   \n",
       "4                    art_gpa_gr_6             0.900475             0.863978   \n",
       "\n",
       "   risk_factor_3_value                                              model  \\\n",
       "0             0.876151  LogisticRegression(C=1.0, class_weight=None, d...   \n",
       "1             0.760293  LogisticRegression(C=1.0, class_weight=None, d...   \n",
       "2             0.789732  LogisticRegression(C=1.0, class_weight=None, d...   \n",
       "3             0.766150  LogisticRegression(C=1.0, class_weight=None, d...   \n",
       "4             0.803051  LogisticRegression(C=1.0, class_weight=None, d...   \n",
       "\n",
       "                                          model_file  \n",
       "0  08_12_2016_grade_7_param_set_17_logit_ht_19082...  \n",
       "1  08_12_2016_grade_7_param_set_17_logit_ht_19082...  \n",
       "2  08_12_2016_grade_7_param_set_17_logit_ht_19082...  \n",
       "3  08_12_2016_grade_7_param_set_17_logit_ht_19082...  \n",
       "4  08_12_2016_grade_7_param_set_17_logit_ht_19082...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 18.6 ms\n"
     ]
    }
   ],
   "source": [
    "individual_scores_factors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 26 s\n"
     ]
    }
   ],
   "source": [
    "def read_in_model(filename, model_name,\n",
    "        pkl_dir = '/mnt/data/mvesc/Models_Results/pkls'):\n",
    "    full_filename = filename +'_' + model_name + '.pkl'\n",
    "    with open(os.path.join(pkl_dir, full_filename), 'rb') as model:\n",
    "        model_pkl = pickle.load(model)\n",
    "    clf, options = model_pkl['estimator'], model_pkl['model_options']\n",
    "    return clf, options\n",
    "\n",
    "def build_test_feature_set(options, current_year = 2016):\n",
    "    # get student list of 2016 students in specified cohort grade level\n",
    "    with postgres_pgconnection_generator() as connection:\n",
    "        cohort = options['cohort_grade_level_begin']\n",
    "        test_outcomes = read_table_to_df(connection,\n",
    "            table_name = 'outcome', schema = 'model', nrows = -1,\n",
    "            columns = ['student_lookup', 'current_students', cohort])\n",
    "        test_outcomes.dropna(subset=['current_students', cohort], inplace=True)\n",
    "        test_outcomes = pd.DataFrame(test_outcomes.student_lookup[\n",
    "            test_outcomes[cohort] == current_year])\n",
    "\n",
    "        for table, column_names in options['features_included'].items():\n",
    "            features = read_table_to_df(connection, table_name = table,\n",
    "                schema = 'model', nrows = -1,\n",
    "                columns=(['student_lookup'] + column_names))\n",
    "            # join to only keep features for current_students\n",
    "            test_outcomes = pd.merge(test_outcomes, features,\n",
    "                how = 'left', on = 'student_lookup')\n",
    "\n",
    "    # build dataframe containing student_lookup\n",
    "    # and all features as numeric non-categorical values\n",
    "    test_outcomes.set_index('student_lookup', inplace=True)\n",
    "    test_outcomes_raw = test_outcomes\n",
    "    test_outcomes = df2num(test_outcomes_raw, drop_reference = False,\n",
    "        dummify = True, drop_entirely_null = False)\n",
    "    return test_outcomes_raw, test_outcomes\n",
    "\n",
    "def test_impute_and_scale(test_outcomes, options):\n",
    "    all_past_data = build_outcomes_plus_features(options)\n",
    "    train, val, val = temporal_cohort_test_split(all_past_data,\n",
    "            options['cohort_grade_level_begin'],\n",
    "            options['cohorts_test'], options['cohorts_val'],\n",
    "            options['cohorts_training'])\n",
    "    exclude = set((options['outcome_name'],\n",
    "                options['cohort_grade_level_begin']))\n",
    "    train = train.drop([options['outcome_name'],\n",
    "            options['cohort_grade_level_begin']],axis=1)\n",
    "    val = val.drop([options['outcome_name'],\n",
    "            options['cohort_grade_level_begin']],axis=1)\n",
    "\n",
    "    category_missing = [col for col in train.columns if\n",
    "                    col not in test_outcomes.columns]\n",
    "    for col in category_missing:\n",
    "        test_outcomes[col] = 0\n",
    "    test_outcomes = test_outcomes.filter(train.columns)\n",
    "\n",
    "    # imputation for missing values in features\n",
    "    train, val, test_outcomes = impute_missing_values(train, val, test_outcomes,\n",
    "        options['missing_impute_strategy'])\n",
    "\n",
    "    # feature scaling\n",
    "    train, val, test_outcomes = scale_features(train, val, test_outcomes,\n",
    "        options['feature_scaling'])\n",
    "\n",
    "    assert (all(train.columns == test_outcomes.columns)),\\\n",
    "        \"train and current_students have different columns\"\n",
    "    return test_outcomes\n",
    "\n",
    "def make_and_save_predictions(future_predictions, clf, filename):\n",
    "    # generate soft predictions\n",
    "    if hasattr(clf, \"predict_proba\"):\n",
    "        future_set_scores = clf.predict_proba(future_predictions)[:,1]\n",
    "    else:\n",
    "        future_set_scores = clf.decision_function(future_predictions)\n",
    "\n",
    "    saved_outputs = {\n",
    "        'file_name' : filename,\n",
    "        'future_index' : future_predictions.index,\n",
    "        'future_scores' : future_set_scores,\n",
    "        'future_preds' : clf.predict(future_predictions)\n",
    "    }\n",
    "    #write_scores_to_db(saved_outputs, importance_scores = False)\n",
    "    return(saved_outputs)\n",
    "\n",
    "def topK_features_logit(model, data, feature_names, topK=3):\n",
    "    importances = np.transpose(model.coef_)[:, 0]*data\n",
    "    indices = importances.argsort()\n",
    "    indices = indices[::-1]\n",
    "    #print(indices[:3])\n",
    "    return(list(np.array(feature_names)[indices[:topK]]))\n",
    "\n",
    "def risk_score2level(score, percentiles, risk_levels):\n",
    "    ind = (percentiles>score).sum()\n",
    "    return(risk_levels[ind])\n",
    "\n",
    "parser = OptionParser()\n",
    "parser.add_option('-f','--filename', dest='filename_list',\n",
    "    help=\"filename for model to generate predictions\",\n",
    "    action=\"append\")\n",
    "(options, args) = parser.parse_args()\n",
    "\n",
    "filename_list = ['08_12_2016_grade_7_param_set_17_logit_ht_19082']\n",
    "topK = 3\n",
    "schema, table = 'model', 'individual_risk_scores_factors'\n",
    "dir_pkls = '/mnt/data/mvesc/Models_Results/pkls'\n",
    "if_exists = 'replace'\n",
    "random_seed = 62571\n",
    "\n",
    "#if options.filename_list:\n",
    "#    filename_list = options.filename_list\n",
    "\n",
    "for filename in filename_list:\n",
    "    # load saved model\n",
    "    model_name = filename.split('_')[-3]\n",
    "    clf, options = read_in_model(filename, model_name)\n",
    "    \n",
    "    # fetch and process feature data\n",
    "    features_raw, feaures_num = build_test_feature_set(options)\n",
    "    features_processed = test_impute_and_scale(feaures_num, options)\n",
    "    \n",
    "    # predict and find top factors\n",
    "    risk_probas = clf.predict_proba(features_processed)[:,1]\n",
    "    predictions = clf.predict(features_processed)\n",
    "    top_individual_features = []\n",
    "    for i in range(features_processed.shape[0]):\n",
    "        x = np.array(features_processed.iloc[i, :])\n",
    "        top_individual_features.append(topK_features_logit(clf, x, features_processed.columns, topK=topK))\n",
    "\n",
    "    top_risk_factor_names = ['risk_factor_'+str(i) for i in range(1, topK+1)]\n",
    "    top_individual_features = pd.DataFrame(top_individual_features, \n",
    "                                           columns=top_risk_factor_names)\n",
    "\n",
    "    # individual risk score, level & factors\n",
    "    individual_scores_factors = pd.DataFrame()\n",
    "    individual_scores_factors['student_lookup'] = features_raw.index\n",
    "    individual_scores_factors['risk_score'] =  risk_probas\n",
    "    \n",
    "    percentiles = individual_scores_factors.risk_score.quantile(q=[0.95, 0.85, 0.70])\n",
    "    risk_levels = ['High', 'Medium', 'Low', 'Safe']\n",
    "    student_risk_levels = [risk_score2level(s, percentiles, risk_levels) for s in individual_scores_factors.risk_score]\n",
    "    individual_scores_factors['risk_level'] = student_risk_levels\n",
    "    individual_scores_factors = pd.concat([individual_scores_factors, top_individual_features], axis=1)\n",
    "\n",
    "    top_feature_values = {'risk_factor_'+str(i):[] for i in range(1, topK+1)}\n",
    "    for risk_i in top_feature_values:\n",
    "        for student_i in range(features_processed.shape[0]):\n",
    "            column_in_features_processed = individual_scores_factors.ix[student_i, risk_i]\n",
    "            top_feature_values[risk_i].append(features_processed[column_in_features_processed].iloc[student_i])\n",
    "    top_feature_values = pd.DataFrame(top_feature_values)\n",
    "    top_feature_values = top_feature_values.rename(columns={x:x+'_value' for x in top_feature_values.columns})\n",
    "    individual_scores_factors = pd.concat([individual_scores_factors, top_feature_values], axis=1)\n",
    "\n",
    "    # model and its file name\n",
    "    individual_scores_factors['model'] = model_name\n",
    "    individual_scores_factors['model_file'] = filename\n",
    "    individual_scores_factors.sort_values(by=['risk_score'],inplace=True, ascending=False)\n",
    "    eng = postgres_engine_generator()\n",
    "    individual_scores_factors.to_sql(table, eng, schema = schema, if_exists=if_exists, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>student_lookup</th>\n",
       "      <th>risk_score</th>\n",
       "      <th>risk_level</th>\n",
       "      <th>risk_factor_1</th>\n",
       "      <th>risk_factor_2</th>\n",
       "      <th>risk_factor_3</th>\n",
       "      <th>risk_factor_1_value</th>\n",
       "      <th>risk_factor_2_value</th>\n",
       "      <th>risk_factor_3_value</th>\n",
       "      <th>model</th>\n",
       "      <th>model_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1201</th>\n",
       "      <td>23990.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>High</td>\n",
       "      <td>stem_gpa_gr_6</td>\n",
       "      <td>gpa_district_gr_6</td>\n",
       "      <td>mid_year_withdraw_gr_5_nan</td>\n",
       "      <td>-18.461538</td>\n",
       "      <td>-8.005487</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>logit</td>\n",
       "      <td>08_12_2016_grade_7_param_set_17_logit_ht_19082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1144</th>\n",
       "      <td>23779.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>High</td>\n",
       "      <td>stem_gpa_gr_6</td>\n",
       "      <td>gpa_district_gr_6</td>\n",
       "      <td>mid_year_withdraw_gr_5_nan</td>\n",
       "      <td>-18.461538</td>\n",
       "      <td>-8.005487</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>logit</td>\n",
       "      <td>08_12_2016_grade_7_param_set_17_logit_ht_19082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27587.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>High</td>\n",
       "      <td>stem_gpa_gr_6</td>\n",
       "      <td>gpa_district_gr_6</td>\n",
       "      <td>mid_year_withdraw_gr_5_nan</td>\n",
       "      <td>-18.461538</td>\n",
       "      <td>-4.386391</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>logit</td>\n",
       "      <td>08_12_2016_grade_7_param_set_17_logit_ht_19082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>13237.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>High</td>\n",
       "      <td>stem_gpa_gr_6</td>\n",
       "      <td>gpa_district_gr_6</td>\n",
       "      <td>read_normalized_gr_3</td>\n",
       "      <td>-17.094017</td>\n",
       "      <td>-6.002796</td>\n",
       "      <td>-2.571315</td>\n",
       "      <td>logit</td>\n",
       "      <td>08_12_2016_grade_7_param_set_17_logit_ht_19082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1641</th>\n",
       "      <td>25517.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>High</td>\n",
       "      <td>stem_gpa_gr_6</td>\n",
       "      <td>gpa_district_gr_6</td>\n",
       "      <td>mid_year_withdraw_gr_5_nan</td>\n",
       "      <td>-14.153846</td>\n",
       "      <td>-4.704481</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>logit</td>\n",
       "      <td>08_12_2016_grade_7_param_set_17_logit_ht_19082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1316</th>\n",
       "      <td>24139.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>High</td>\n",
       "      <td>stem_gpa_gr_6</td>\n",
       "      <td>gpa_district_gr_6</td>\n",
       "      <td>read_normalized_gr_3</td>\n",
       "      <td>-18.461538</td>\n",
       "      <td>-6.308971</td>\n",
       "      <td>-1.191980</td>\n",
       "      <td>logit</td>\n",
       "      <td>08_12_2016_grade_7_param_set_17_logit_ht_19082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2158</th>\n",
       "      <td>28439.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>High</td>\n",
       "      <td>stem_gpa_gr_6</td>\n",
       "      <td>gpa_district_gr_6</td>\n",
       "      <td>mid_year_withdraw_gr_5_nan</td>\n",
       "      <td>-18.461538</td>\n",
       "      <td>-4.715497</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>logit</td>\n",
       "      <td>08_12_2016_grade_7_param_set_17_logit_ht_19082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1683</th>\n",
       "      <td>25916.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>High</td>\n",
       "      <td>stem_gpa_gr_6</td>\n",
       "      <td>gpa_district_gr_6</td>\n",
       "      <td>mid_year_withdraw_gr_5_nan</td>\n",
       "      <td>-18.461538</td>\n",
       "      <td>-4.956846</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>logit</td>\n",
       "      <td>08_12_2016_grade_7_param_set_17_logit_ht_19082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1272</th>\n",
       "      <td>24088.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>High</td>\n",
       "      <td>stem_gpa_gr_6</td>\n",
       "      <td>gpa_district_gr_6</td>\n",
       "      <td>mid_year_withdraw_gr_5_nan</td>\n",
       "      <td>-15.384615</td>\n",
       "      <td>-7.411328</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>logit</td>\n",
       "      <td>08_12_2016_grade_7_param_set_17_logit_ht_19082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2327</th>\n",
       "      <td>33950.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>High</td>\n",
       "      <td>stem_gpa_gr_6</td>\n",
       "      <td>mid_year_withdraw_gr_5_nan</td>\n",
       "      <td>gpa_district_gr_6</td>\n",
       "      <td>-14.461538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.608686</td>\n",
       "      <td>logit</td>\n",
       "      <td>08_12_2016_grade_7_param_set_17_logit_ht_19082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>11929.0</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>Safe</td>\n",
       "      <td>mid_year_withdraw_gr_5_nan</td>\n",
       "      <td>num_art_classes_gr_6</td>\n",
       "      <td>gpa_gr_5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>logit</td>\n",
       "      <td>08_12_2016_grade_7_param_set_17_logit_ht_19082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1881</th>\n",
       "      <td>27712.0</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>Safe</td>\n",
       "      <td>mid_year_withdraw_gr_5_nan</td>\n",
       "      <td>gpa_district_gr_5</td>\n",
       "      <td>gpa_gr_5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.899746</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>logit</td>\n",
       "      <td>08_12_2016_grade_7_param_set_17_logit_ht_19082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2242</th>\n",
       "      <td>28715.0</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>Safe</td>\n",
       "      <td>gpa_gr_5</td>\n",
       "      <td>gpa_district_gr_5</td>\n",
       "      <td>avg_district_change_to_gr_6</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.706435</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>logit</td>\n",
       "      <td>08_12_2016_grade_7_param_set_17_logit_ht_19082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>28143.0</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>Safe</td>\n",
       "      <td>mid_year_withdraw_gr_5_nan</td>\n",
       "      <td>gpa_district_gr_5</td>\n",
       "      <td>gpa_gr_5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.899746</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>logit</td>\n",
       "      <td>08_12_2016_grade_7_param_set_17_logit_ht_19082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>11931.0</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>Safe</td>\n",
       "      <td>mid_year_withdraw_gr_5_nan</td>\n",
       "      <td>num_art_classes_gr_6</td>\n",
       "      <td>n_cities_to_gr_6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>logit</td>\n",
       "      <td>08_12_2016_grade_7_param_set_17_logit_ht_19082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>13255.0</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>Safe</td>\n",
       "      <td>mid_year_withdraw_gr_5_nan</td>\n",
       "      <td>num_art_classes_gr_6</td>\n",
       "      <td>n_cities_to_gr_6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>logit</td>\n",
       "      <td>08_12_2016_grade_7_param_set_17_logit_ht_19082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1740</th>\n",
       "      <td>26155.0</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>Safe</td>\n",
       "      <td>mid_year_withdraw_gr_5_nan</td>\n",
       "      <td>gpa_district_gr_5</td>\n",
       "      <td>gpa_gr_5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.899746</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>logit</td>\n",
       "      <td>08_12_2016_grade_7_param_set_17_logit_ht_19082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>11375.0</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>Safe</td>\n",
       "      <td>mid_year_withdraw_gr_5_nan</td>\n",
       "      <td>num_art_classes_gr_6</td>\n",
       "      <td>gpa_gr_5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>logit</td>\n",
       "      <td>08_12_2016_grade_7_param_set_17_logit_ht_19082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>9898.0</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>Safe</td>\n",
       "      <td>mid_year_withdraw_gr_5_nan</td>\n",
       "      <td>n_cities_to_gr_6</td>\n",
       "      <td>gpa_gr_5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>logit</td>\n",
       "      <td>08_12_2016_grade_7_param_set_17_logit_ht_19082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1891</th>\n",
       "      <td>27743.0</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>Safe</td>\n",
       "      <td>mid_year_withdraw_gr_5_nan</td>\n",
       "      <td>gpa_district_gr_5</td>\n",
       "      <td>gpa_gr_5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.899746</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>logit</td>\n",
       "      <td>08_12_2016_grade_7_param_set_17_logit_ht_19082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2379 rows  11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      student_lookup  risk_score risk_level               risk_factor_1  \\\n",
       "1201         23990.0    1.000000       High               stem_gpa_gr_6   \n",
       "1144         23779.0    1.000000       High               stem_gpa_gr_6   \n",
       "0            27587.0    1.000000       High               stem_gpa_gr_6   \n",
       "811          13237.0    1.000000       High               stem_gpa_gr_6   \n",
       "1641         25517.0    1.000000       High               stem_gpa_gr_6   \n",
       "1316         24139.0    1.000000       High               stem_gpa_gr_6   \n",
       "2158         28439.0    1.000000       High               stem_gpa_gr_6   \n",
       "1683         25916.0    1.000000       High               stem_gpa_gr_6   \n",
       "1272         24088.0    1.000000       High               stem_gpa_gr_6   \n",
       "2327         33950.0    1.000000       High               stem_gpa_gr_6   \n",
       "...              ...         ...        ...                         ...   \n",
       "743          11929.0    0.000017       Safe  mid_year_withdraw_gr_5_nan   \n",
       "1881         27712.0    0.000016       Safe  mid_year_withdraw_gr_5_nan   \n",
       "2242         28715.0    0.000015       Safe                    gpa_gr_5   \n",
       "2002         28143.0    0.000014       Safe  mid_year_withdraw_gr_5_nan   \n",
       "745          11931.0    0.000013       Safe  mid_year_withdraw_gr_5_nan   \n",
       "829          13255.0    0.000013       Safe  mid_year_withdraw_gr_5_nan   \n",
       "1740         26155.0    0.000011       Safe  mid_year_withdraw_gr_5_nan   \n",
       "673          11375.0    0.000011       Safe  mid_year_withdraw_gr_5_nan   \n",
       "499           9898.0    0.000011       Safe  mid_year_withdraw_gr_5_nan   \n",
       "1891         27743.0    0.000004       Safe  mid_year_withdraw_gr_5_nan   \n",
       "\n",
       "                   risk_factor_2                risk_factor_3  \\\n",
       "1201           gpa_district_gr_6   mid_year_withdraw_gr_5_nan   \n",
       "1144           gpa_district_gr_6   mid_year_withdraw_gr_5_nan   \n",
       "0              gpa_district_gr_6   mid_year_withdraw_gr_5_nan   \n",
       "811            gpa_district_gr_6         read_normalized_gr_3   \n",
       "1641           gpa_district_gr_6   mid_year_withdraw_gr_5_nan   \n",
       "1316           gpa_district_gr_6         read_normalized_gr_3   \n",
       "2158           gpa_district_gr_6   mid_year_withdraw_gr_5_nan   \n",
       "1683           gpa_district_gr_6   mid_year_withdraw_gr_5_nan   \n",
       "1272           gpa_district_gr_6   mid_year_withdraw_gr_5_nan   \n",
       "2327  mid_year_withdraw_gr_5_nan            gpa_district_gr_6   \n",
       "...                          ...                          ...   \n",
       "743         num_art_classes_gr_6                     gpa_gr_5   \n",
       "1881           gpa_district_gr_5                     gpa_gr_5   \n",
       "2242           gpa_district_gr_5  avg_district_change_to_gr_6   \n",
       "2002           gpa_district_gr_5                     gpa_gr_5   \n",
       "745         num_art_classes_gr_6             n_cities_to_gr_6   \n",
       "829         num_art_classes_gr_6             n_cities_to_gr_6   \n",
       "1740           gpa_district_gr_5                     gpa_gr_5   \n",
       "673         num_art_classes_gr_6                     gpa_gr_5   \n",
       "499             n_cities_to_gr_6                     gpa_gr_5   \n",
       "1891           gpa_district_gr_5                     gpa_gr_5   \n",
       "\n",
       "      risk_factor_1_value  risk_factor_2_value  risk_factor_3_value  model  \\\n",
       "1201           -18.461538            -8.005487             1.000000  logit   \n",
       "1144           -18.461538            -8.005487             1.000000  logit   \n",
       "0              -18.461538            -4.386391             1.000000  logit   \n",
       "811            -17.094017            -6.002796            -2.571315  logit   \n",
       "1641           -14.153846            -4.704481             1.000000  logit   \n",
       "1316           -18.461538            -6.308971            -1.191980  logit   \n",
       "2158           -18.461538            -4.715497             1.000000  logit   \n",
       "1683           -18.461538            -4.956846             1.000000  logit   \n",
       "1272           -15.384615            -7.411328             1.000000  logit   \n",
       "2327           -14.461538             1.000000            -2.608686  logit   \n",
       "...                   ...                  ...                  ...    ...   \n",
       "743              1.000000             1.000000             0.750000  logit   \n",
       "1881             1.000000             0.899746             0.750000  logit   \n",
       "2242             0.750000             0.706435             0.250000  logit   \n",
       "2002             1.000000             0.899746             0.750000  logit   \n",
       "745              1.000000             1.000000             1.000000  logit   \n",
       "829              1.000000             1.000000             1.000000  logit   \n",
       "1740             1.000000             0.899746             0.750000  logit   \n",
       "673              1.000000             1.000000             0.750000  logit   \n",
       "499              1.000000             1.000000             0.750000  logit   \n",
       "1891             1.000000             0.899746             0.750000  logit   \n",
       "\n",
       "                                          model_file  \n",
       "1201  08_12_2016_grade_7_param_set_17_logit_ht_19082  \n",
       "1144  08_12_2016_grade_7_param_set_17_logit_ht_19082  \n",
       "0     08_12_2016_grade_7_param_set_17_logit_ht_19082  \n",
       "811   08_12_2016_grade_7_param_set_17_logit_ht_19082  \n",
       "1641  08_12_2016_grade_7_param_set_17_logit_ht_19082  \n",
       "1316  08_12_2016_grade_7_param_set_17_logit_ht_19082  \n",
       "2158  08_12_2016_grade_7_param_set_17_logit_ht_19082  \n",
       "1683  08_12_2016_grade_7_param_set_17_logit_ht_19082  \n",
       "1272  08_12_2016_grade_7_param_set_17_logit_ht_19082  \n",
       "2327  08_12_2016_grade_7_param_set_17_logit_ht_19082  \n",
       "...                                              ...  \n",
       "743   08_12_2016_grade_7_param_set_17_logit_ht_19082  \n",
       "1881  08_12_2016_grade_7_param_set_17_logit_ht_19082  \n",
       "2242  08_12_2016_grade_7_param_set_17_logit_ht_19082  \n",
       "2002  08_12_2016_grade_7_param_set_17_logit_ht_19082  \n",
       "745   08_12_2016_grade_7_param_set_17_logit_ht_19082  \n",
       "829   08_12_2016_grade_7_param_set_17_logit_ht_19082  \n",
       "1740  08_12_2016_grade_7_param_set_17_logit_ht_19082  \n",
       "673   08_12_2016_grade_7_param_set_17_logit_ht_19082  \n",
       "499   08_12_2016_grade_7_param_set_17_logit_ht_19082  \n",
       "1891  08_12_2016_grade_7_param_set_17_logit_ht_19082  \n",
       "\n",
       "[2379 rows x 11 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 33.8 ms\n"
     ]
    }
   ],
   "source": [
    "individual_scores_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>student_lookup</th>\n",
       "      <th>risk_score</th>\n",
       "      <th>risk_level</th>\n",
       "      <th>risk_factor_1</th>\n",
       "      <th>risk_factor_2</th>\n",
       "      <th>risk_factor_3</th>\n",
       "      <th>risk_factor_1_value</th>\n",
       "      <th>risk_factor_2_value</th>\n",
       "      <th>risk_factor_3_value</th>\n",
       "      <th>model</th>\n",
       "      <th>model_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1201</th>\n",
       "      <td>23990.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>High</td>\n",
       "      <td>stem_gpa_gr_6</td>\n",
       "      <td>gpa_district_gr_6</td>\n",
       "      <td>mid_year_withdraw_gr_5_nan</td>\n",
       "      <td>-18.461538</td>\n",
       "      <td>-8.005487</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>logit</td>\n",
       "      <td>08_12_2016_grade_7_param_set_17_logit_ht_19082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1144</th>\n",
       "      <td>23779.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>High</td>\n",
       "      <td>stem_gpa_gr_6</td>\n",
       "      <td>gpa_district_gr_6</td>\n",
       "      <td>mid_year_withdraw_gr_5_nan</td>\n",
       "      <td>-18.461538</td>\n",
       "      <td>-8.005487</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>logit</td>\n",
       "      <td>08_12_2016_grade_7_param_set_17_logit_ht_19082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27587.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>High</td>\n",
       "      <td>stem_gpa_gr_6</td>\n",
       "      <td>gpa_district_gr_6</td>\n",
       "      <td>mid_year_withdraw_gr_5_nan</td>\n",
       "      <td>-18.461538</td>\n",
       "      <td>-4.386391</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>logit</td>\n",
       "      <td>08_12_2016_grade_7_param_set_17_logit_ht_19082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>13237.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>High</td>\n",
       "      <td>stem_gpa_gr_6</td>\n",
       "      <td>gpa_district_gr_6</td>\n",
       "      <td>read_normalized_gr_3</td>\n",
       "      <td>-17.094017</td>\n",
       "      <td>-6.002796</td>\n",
       "      <td>-2.571315</td>\n",
       "      <td>logit</td>\n",
       "      <td>08_12_2016_grade_7_param_set_17_logit_ht_19082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1641</th>\n",
       "      <td>25517.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>High</td>\n",
       "      <td>stem_gpa_gr_6</td>\n",
       "      <td>gpa_district_gr_6</td>\n",
       "      <td>mid_year_withdraw_gr_5_nan</td>\n",
       "      <td>-14.153846</td>\n",
       "      <td>-4.704481</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>logit</td>\n",
       "      <td>08_12_2016_grade_7_param_set_17_logit_ht_19082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1316</th>\n",
       "      <td>24139.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>High</td>\n",
       "      <td>stem_gpa_gr_6</td>\n",
       "      <td>gpa_district_gr_6</td>\n",
       "      <td>read_normalized_gr_3</td>\n",
       "      <td>-18.461538</td>\n",
       "      <td>-6.308971</td>\n",
       "      <td>-1.191980</td>\n",
       "      <td>logit</td>\n",
       "      <td>08_12_2016_grade_7_param_set_17_logit_ht_19082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2158</th>\n",
       "      <td>28439.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>High</td>\n",
       "      <td>stem_gpa_gr_6</td>\n",
       "      <td>gpa_district_gr_6</td>\n",
       "      <td>mid_year_withdraw_gr_5_nan</td>\n",
       "      <td>-18.461538</td>\n",
       "      <td>-4.715497</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>logit</td>\n",
       "      <td>08_12_2016_grade_7_param_set_17_logit_ht_19082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1683</th>\n",
       "      <td>25916.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>High</td>\n",
       "      <td>stem_gpa_gr_6</td>\n",
       "      <td>gpa_district_gr_6</td>\n",
       "      <td>mid_year_withdraw_gr_5_nan</td>\n",
       "      <td>-18.461538</td>\n",
       "      <td>-4.956846</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>logit</td>\n",
       "      <td>08_12_2016_grade_7_param_set_17_logit_ht_19082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1272</th>\n",
       "      <td>24088.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>High</td>\n",
       "      <td>stem_gpa_gr_6</td>\n",
       "      <td>gpa_district_gr_6</td>\n",
       "      <td>mid_year_withdraw_gr_5_nan</td>\n",
       "      <td>-15.384615</td>\n",
       "      <td>-7.411328</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>logit</td>\n",
       "      <td>08_12_2016_grade_7_param_set_17_logit_ht_19082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2327</th>\n",
       "      <td>33950.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>High</td>\n",
       "      <td>stem_gpa_gr_6</td>\n",
       "      <td>mid_year_withdraw_gr_5_nan</td>\n",
       "      <td>gpa_district_gr_6</td>\n",
       "      <td>-14.461538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.608686</td>\n",
       "      <td>logit</td>\n",
       "      <td>08_12_2016_grade_7_param_set_17_logit_ht_19082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>11929.0</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>Safe</td>\n",
       "      <td>mid_year_withdraw_gr_5_nan</td>\n",
       "      <td>num_art_classes_gr_6</td>\n",
       "      <td>gpa_gr_5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>logit</td>\n",
       "      <td>08_12_2016_grade_7_param_set_17_logit_ht_19082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1881</th>\n",
       "      <td>27712.0</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>Safe</td>\n",
       "      <td>mid_year_withdraw_gr_5_nan</td>\n",
       "      <td>gpa_district_gr_5</td>\n",
       "      <td>gpa_gr_5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.899746</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>logit</td>\n",
       "      <td>08_12_2016_grade_7_param_set_17_logit_ht_19082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2242</th>\n",
       "      <td>28715.0</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>Safe</td>\n",
       "      <td>gpa_gr_5</td>\n",
       "      <td>gpa_district_gr_5</td>\n",
       "      <td>avg_district_change_to_gr_6</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.706435</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>logit</td>\n",
       "      <td>08_12_2016_grade_7_param_set_17_logit_ht_19082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>28143.0</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>Safe</td>\n",
       "      <td>mid_year_withdraw_gr_5_nan</td>\n",
       "      <td>gpa_district_gr_5</td>\n",
       "      <td>gpa_gr_5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.899746</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>logit</td>\n",
       "      <td>08_12_2016_grade_7_param_set_17_logit_ht_19082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>11931.0</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>Safe</td>\n",
       "      <td>mid_year_withdraw_gr_5_nan</td>\n",
       "      <td>num_art_classes_gr_6</td>\n",
       "      <td>n_cities_to_gr_6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>logit</td>\n",
       "      <td>08_12_2016_grade_7_param_set_17_logit_ht_19082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>13255.0</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>Safe</td>\n",
       "      <td>mid_year_withdraw_gr_5_nan</td>\n",
       "      <td>num_art_classes_gr_6</td>\n",
       "      <td>n_cities_to_gr_6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>logit</td>\n",
       "      <td>08_12_2016_grade_7_param_set_17_logit_ht_19082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1740</th>\n",
       "      <td>26155.0</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>Safe</td>\n",
       "      <td>mid_year_withdraw_gr_5_nan</td>\n",
       "      <td>gpa_district_gr_5</td>\n",
       "      <td>gpa_gr_5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.899746</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>logit</td>\n",
       "      <td>08_12_2016_grade_7_param_set_17_logit_ht_19082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>11375.0</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>Safe</td>\n",
       "      <td>mid_year_withdraw_gr_5_nan</td>\n",
       "      <td>num_art_classes_gr_6</td>\n",
       "      <td>gpa_gr_5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>logit</td>\n",
       "      <td>08_12_2016_grade_7_param_set_17_logit_ht_19082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>9898.0</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>Safe</td>\n",
       "      <td>mid_year_withdraw_gr_5_nan</td>\n",
       "      <td>n_cities_to_gr_6</td>\n",
       "      <td>gpa_gr_5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>logit</td>\n",
       "      <td>08_12_2016_grade_7_param_set_17_logit_ht_19082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1891</th>\n",
       "      <td>27743.0</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>Safe</td>\n",
       "      <td>mid_year_withdraw_gr_5_nan</td>\n",
       "      <td>gpa_district_gr_5</td>\n",
       "      <td>gpa_gr_5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.899746</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>logit</td>\n",
       "      <td>08_12_2016_grade_7_param_set_17_logit_ht_19082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2379 rows  11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      student_lookup  risk_score risk_level               risk_factor_1  \\\n",
       "1201         23990.0    1.000000       High               stem_gpa_gr_6   \n",
       "1144         23779.0    1.000000       High               stem_gpa_gr_6   \n",
       "0            27587.0    1.000000       High               stem_gpa_gr_6   \n",
       "811          13237.0    1.000000       High               stem_gpa_gr_6   \n",
       "1641         25517.0    1.000000       High               stem_gpa_gr_6   \n",
       "1316         24139.0    1.000000       High               stem_gpa_gr_6   \n",
       "2158         28439.0    1.000000       High               stem_gpa_gr_6   \n",
       "1683         25916.0    1.000000       High               stem_gpa_gr_6   \n",
       "1272         24088.0    1.000000       High               stem_gpa_gr_6   \n",
       "2327         33950.0    1.000000       High               stem_gpa_gr_6   \n",
       "...              ...         ...        ...                         ...   \n",
       "743          11929.0    0.000017       Safe  mid_year_withdraw_gr_5_nan   \n",
       "1881         27712.0    0.000016       Safe  mid_year_withdraw_gr_5_nan   \n",
       "2242         28715.0    0.000015       Safe                    gpa_gr_5   \n",
       "2002         28143.0    0.000014       Safe  mid_year_withdraw_gr_5_nan   \n",
       "745          11931.0    0.000013       Safe  mid_year_withdraw_gr_5_nan   \n",
       "829          13255.0    0.000013       Safe  mid_year_withdraw_gr_5_nan   \n",
       "1740         26155.0    0.000011       Safe  mid_year_withdraw_gr_5_nan   \n",
       "673          11375.0    0.000011       Safe  mid_year_withdraw_gr_5_nan   \n",
       "499           9898.0    0.000011       Safe  mid_year_withdraw_gr_5_nan   \n",
       "1891         27743.0    0.000004       Safe  mid_year_withdraw_gr_5_nan   \n",
       "\n",
       "                   risk_factor_2                risk_factor_3  \\\n",
       "1201           gpa_district_gr_6   mid_year_withdraw_gr_5_nan   \n",
       "1144           gpa_district_gr_6   mid_year_withdraw_gr_5_nan   \n",
       "0              gpa_district_gr_6   mid_year_withdraw_gr_5_nan   \n",
       "811            gpa_district_gr_6         read_normalized_gr_3   \n",
       "1641           gpa_district_gr_6   mid_year_withdraw_gr_5_nan   \n",
       "1316           gpa_district_gr_6         read_normalized_gr_3   \n",
       "2158           gpa_district_gr_6   mid_year_withdraw_gr_5_nan   \n",
       "1683           gpa_district_gr_6   mid_year_withdraw_gr_5_nan   \n",
       "1272           gpa_district_gr_6   mid_year_withdraw_gr_5_nan   \n",
       "2327  mid_year_withdraw_gr_5_nan            gpa_district_gr_6   \n",
       "...                          ...                          ...   \n",
       "743         num_art_classes_gr_6                     gpa_gr_5   \n",
       "1881           gpa_district_gr_5                     gpa_gr_5   \n",
       "2242           gpa_district_gr_5  avg_district_change_to_gr_6   \n",
       "2002           gpa_district_gr_5                     gpa_gr_5   \n",
       "745         num_art_classes_gr_6             n_cities_to_gr_6   \n",
       "829         num_art_classes_gr_6             n_cities_to_gr_6   \n",
       "1740           gpa_district_gr_5                     gpa_gr_5   \n",
       "673         num_art_classes_gr_6                     gpa_gr_5   \n",
       "499             n_cities_to_gr_6                     gpa_gr_5   \n",
       "1891           gpa_district_gr_5                     gpa_gr_5   \n",
       "\n",
       "      risk_factor_1_value  risk_factor_2_value  risk_factor_3_value  model  \\\n",
       "1201           -18.461538            -8.005487             1.000000  logit   \n",
       "1144           -18.461538            -8.005487             1.000000  logit   \n",
       "0              -18.461538            -4.386391             1.000000  logit   \n",
       "811            -17.094017            -6.002796            -2.571315  logit   \n",
       "1641           -14.153846            -4.704481             1.000000  logit   \n",
       "1316           -18.461538            -6.308971            -1.191980  logit   \n",
       "2158           -18.461538            -4.715497             1.000000  logit   \n",
       "1683           -18.461538            -4.956846             1.000000  logit   \n",
       "1272           -15.384615            -7.411328             1.000000  logit   \n",
       "2327           -14.461538             1.000000            -2.608686  logit   \n",
       "...                   ...                  ...                  ...    ...   \n",
       "743              1.000000             1.000000             0.750000  logit   \n",
       "1881             1.000000             0.899746             0.750000  logit   \n",
       "2242             0.750000             0.706435             0.250000  logit   \n",
       "2002             1.000000             0.899746             0.750000  logit   \n",
       "745              1.000000             1.000000             1.000000  logit   \n",
       "829              1.000000             1.000000             1.000000  logit   \n",
       "1740             1.000000             0.899746             0.750000  logit   \n",
       "673              1.000000             1.000000             0.750000  logit   \n",
       "499              1.000000             1.000000             0.750000  logit   \n",
       "1891             1.000000             0.899746             0.750000  logit   \n",
       "\n",
       "                                          model_file  \n",
       "1201  08_12_2016_grade_7_param_set_17_logit_ht_19082  \n",
       "1144  08_12_2016_grade_7_param_set_17_logit_ht_19082  \n",
       "0     08_12_2016_grade_7_param_set_17_logit_ht_19082  \n",
       "811   08_12_2016_grade_7_param_set_17_logit_ht_19082  \n",
       "1641  08_12_2016_grade_7_param_set_17_logit_ht_19082  \n",
       "1316  08_12_2016_grade_7_param_set_17_logit_ht_19082  \n",
       "2158  08_12_2016_grade_7_param_set_17_logit_ht_19082  \n",
       "1683  08_12_2016_grade_7_param_set_17_logit_ht_19082  \n",
       "1272  08_12_2016_grade_7_param_set_17_logit_ht_19082  \n",
       "2327  08_12_2016_grade_7_param_set_17_logit_ht_19082  \n",
       "...                                              ...  \n",
       "743   08_12_2016_grade_7_param_set_17_logit_ht_19082  \n",
       "1881  08_12_2016_grade_7_param_set_17_logit_ht_19082  \n",
       "2242  08_12_2016_grade_7_param_set_17_logit_ht_19082  \n",
       "2002  08_12_2016_grade_7_param_set_17_logit_ht_19082  \n",
       "745   08_12_2016_grade_7_param_set_17_logit_ht_19082  \n",
       "829   08_12_2016_grade_7_param_set_17_logit_ht_19082  \n",
       "1740  08_12_2016_grade_7_param_set_17_logit_ht_19082  \n",
       "673   08_12_2016_grade_7_param_set_17_logit_ht_19082  \n",
       "499   08_12_2016_grade_7_param_set_17_logit_ht_19082  \n",
       "1891  08_12_2016_grade_7_param_set_17_logit_ht_19082  \n",
       "\n",
       "[2379 rows x 11 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 34.1 ms\n"
     ]
    }
   ],
   "source": [
    "individual_scores_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'High',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Medium',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Low',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " 'Safe',\n",
       " ...]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 13.8 ms\n"
     ]
    }
   ],
   "source": [
    "student_risk_levels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
